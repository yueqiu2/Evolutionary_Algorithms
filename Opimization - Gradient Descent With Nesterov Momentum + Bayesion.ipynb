{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yueqiu2/Evolutionary_Algorithms/blob/main/Opimization%20-%20Gradient%20Descent%20With%20Nesterov%20Momentum%20%2B%20Bayesion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box-constrained Derivative-Free optimization"
      ],
      "metadata": {
        "id": "OtcVPNopOfct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the supporting practice notebook, you were guided through the creation of a trust-region model-based derivative free optimization (DFO) algorithm. In this coursework, you will design your own DFO solver. You have complete freedom in what type of DFO algorithm you want to implement - direct, model-based, evolutionary search, ... Since the takeaway is on optimising chemical engineering systems, we operate under the following assumptions:\n",
        "\n",
        "- Evaluations are expensive, meaning that the runtime of the algorithms is limited by a fixed evaluation budget rather than a time budget constraint. However, we will not consider algorithms that take longer than 5 minutes for a budget of 1,000 evaluations. Therefore, it might be a good idea to have your algorithm exit, and return the best value found so far if the timing is nearing the 5 minute mark (see for [example](https://stackoverflow.com/questions/889900/accurate-timing-of-functions-in-python))\n",
        "\n",
        "- We are allowing for budgets ranging between 50 and 1,000 evaluations depending on dimensionalities ranging between 1 and 100\n",
        "\n",
        "- Input bounds for all dimensions are known\n",
        "\n",
        "\n",
        "You are given training functions, but you are encouraged to look for your own; This [website](https://www.sfu.ca/~ssurjano/optimization.html) can be useful. Your implementation will be graded based on the best objective evaluation obtained. You will get a minimum of 60% if your implementation beats all benchmarks (Nelder-Mead, Powell, COBYLA), or if you get within $10^{-4}$ of their objective within the allocated budget, and the rest of your grade will be curved by state-of-the-art solvers. A (very bad) optimizer function is given for illustration.\n",
        "\n",
        "The same marking procedure will then be applied to the testing functions  that you cannot see. This is to make sure that the performance of your optimizer is not 'overfitted' on the training functions. You can prepare for this part by making sure your algorithm still performs well on some of the test functions in the above link. I will make sure that the type of problems remains similar when going from the training to test functions, i.e., I will not test the functions on highly nonconvex / multimodal functions if there are none given as training functions. In the test functions you will get 40% of the overall marks for beating the benchmarks.\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "- Good coding practice: Make sure your function takes the same inputs and outputs as the illustrative optimizer, and make sure that your implementation is flexible enough to handle different dimensionalities. This is **very important**, as otherwise we have no way to test your algorithms, and you will not get marks for those problems (this is easily mitigated by making sure your algorithms run in the training set and follow same inputs and outputs as the sample function).\n",
        "\n",
        "- Stochasticity: You are highly encouraged to set random seeds if your implementation involves stochasticity. This is to make sure that your implementation works as expected when marking.\n",
        "\n",
        "- Packages: please restrict to use numpy, scipy, and GPy. You are allowed to use scipy.optimize to train any surrogates if you wish, but you are not allowed to optimize f() directly, neither to use GPyOpt directly on the functions.\n",
        "\n",
        "- Runtime: For our inexpensive test functions, your algorithm should finish running in **5** minutes, otherwise it will not be graded. This becomes especially relevant if you decide to pursue supplementary workbook B.\n"
      ],
      "metadata": {
        "id": "1txoO0ssIVSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some packages"
      ],
      "metadata": {
        "id": "d13ajElfTkNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPy\n",
        "\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import GPy\n",
        "\n",
        "from scipy.stats import qmc\n",
        "from functools import partial\n",
        "from typing import List, Tuple, Dict\n",
        "from scipy.optimize import minimize, differential_evolution, show_options\n"
      ],
      "metadata": {
        "id": "Z-_CgN8iIVn3",
        "outputId": "b897b351-0a44-4e57-f654-419004f4d818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPy\n",
            "  Downloading GPy-1.10.0.tar.gz (959 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.4/959.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from GPy) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from GPy) (1.16.0)\n",
            "Collecting paramz>=0.9.0 (from GPy)\n",
            "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from GPy) (3.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from GPy) (1.11.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Building wheels for collected packages: GPy, paramz\n",
            "  Building wheel for GPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPy: filename=GPy-1.10.0-cp310-cp310-linux_x86_64.whl size=3410102 sha256=b0be02c4e64957894d894d8cde8464d9524bd5777aa4e2ac4e680f89c6aca883\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/bd/9f/82ab4216eae088cba864ca0dc1d75699bd4bf6823790fb2f77\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102544 sha256=bb410593b020d44b83ed1150d4d8eda991925bfde647c19912c362e585b7f519\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/ef/9c/da9ceef7d0ff5287c24365844fc394852c2b79ac3fcf33bf8b\n",
            "Successfully built GPy paramz\n",
            "Installing collected packages: paramz, GPy\n",
            "Successfully installed GPy-1.10.0 paramz-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some test functions"
      ],
      "metadata": {
        "id": "DPX_0DaUTnKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_d2(x: List[float]) -> float:\n",
        "  '''\n",
        "  f is the \"black-box\" function that needs to be optimized\n",
        "  Input:\n",
        "    x: list of size N_x/ array of size (N_x,)\n",
        "  Output:\n",
        "    float: function evaluation associated with x\n",
        "  '''\n",
        "  x = np.array(x)\n",
        "  return (1-x[0])**2 + 100*(x[1]-x[0]**2)**2\n",
        "\n",
        "def f_d3(x: List[float]) -> float:\n",
        "  '''\n",
        "  f is the \"black-box\" function that needs to be optimized\n",
        "  Input:\n",
        "    x: list of size N_x/ array of size (N_x,)\n",
        "  Output:\n",
        "    float: function evaluation associated with x\n",
        "  '''\n",
        "  x = np.array(x)\n",
        "  return (1-x[0])**2 + 100*(x[1]-x[0]**2)**2 + x[2]**2"
      ],
      "metadata": {
        "id": "naGXPGqQ9E_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dummy optimizer function. Please keep the function name as 'optimizer'"
      ],
      "metadata": {
        "id": "h9bnzLS2UCsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 NAG\n",
        "def NAG(f, bounds, beta, N_x, NAGbudget, initial_x):\n",
        "    # initialize problem\n",
        "    # bounds = np.array(bounds)\n",
        "    x0 = np.array(initial_x)\n",
        "    # x0 = np.add(bounds[:,1], np.add(bounds[:,0],0.1))/2\n",
        "    x      = np.copy(x0)\n",
        "\n",
        "    iter_i = 0\n",
        "    grad_i = 1e-4*10\n",
        "\n",
        "    # optimization loop\n",
        "    v_prev = 0      # initialize at zero to get normal GD-momentum at first step\n",
        "    while np.sum(np.abs(grad_i)) > 1e-4 and iter_i < NAGbudget:\n",
        "        grad_i  = grad_f(f,x,N_x)                                 # compute gradient at current position\n",
        "\n",
        "        x_tilde    = x + beta * v_prev                            # nesterov modified position\n",
        "        grad_tilde = grad_f(f, x_tilde, N_x)                      # compute gradient of function at x_tilde\n",
        "        lr         = line_search(grad_tilde, x, f, A=0.1, B=0.8)  # compute learning rate using line search\n",
        "        v          = nesterov(grad_tilde, v_prev, lr, beta)       # compute momentum\n",
        "\n",
        "        x       = x + v                                           # compute step\n",
        "        v_prev  = v                                               # update previous momentum term\n",
        "        iter_i += 1\n",
        "\n",
        "    # final optimum\n",
        "    minimum = f(x)\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x)\n",
        "    print('Minimal f : ', minimum)\n",
        "    print('Final grad: ', grad_i)\n",
        "\n",
        "    return minimum"
      ],
      "metadata": {
        "id": "6TJAUTEl5Bp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_f(f, x, N_x):\n",
        "    # x = [ ], x.shape = (n,), x.shape[0] = n\n",
        "    # dim = x.shape[0]\n",
        "    # epsilon: square of smallest float\n",
        "    # eps  = np.sqrt(np.finfo(float).eps)\n",
        "    eps = np.sqrt(np.finfo(float).eps)\n",
        "    # original [[a b]]\n",
        "    grad = np.zeros((1,N_x))\n",
        "    # change grad to []\n",
        "    grad = grad[0]\n",
        "\n",
        "    for i in range(N_x):\n",
        "\n",
        "        # e for each dimension with [[]]\n",
        "        e = np.zeros((1,N_x))\n",
        "        e[0,i] = eps\n",
        "        # change e to []\n",
        "        e = e[0]\n",
        "\n",
        "        # gradient\n",
        "        # already change x and e to []\n",
        "        grad_approx = (f(x+e/2)-f(x-e/2))/eps\n",
        "        # change from grad[0,i] to grad[i]\n",
        "        grad[i] = grad_approx\n",
        "\n",
        "    return grad # grad in []"
      ],
      "metadata": {
        "id": "Y8OR-mwT5L6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def line_search(grad_i, x, f, A, B):\n",
        "    iter = 0\n",
        "    lr   = 1\n",
        "\n",
        "    while f(x - lr*grad_i) > f(x) - A*lr*np.dot(grad_i, grad_i):\n",
        "        lr    = B*lr\n",
        "        iter += 1\n",
        "\n",
        "    return lr  # only return lr"
      ],
      "metadata": {
        "id": "jK_h86D15SAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov(grad_tilde, v_prev, lr, beta):\n",
        "    '''\n",
        "    Momentum function\n",
        "    INPUTS:\n",
        "        grad_tilde  : Gradient of function at nesterov modified position\n",
        "        v_prev      : velocity value at the previous position\n",
        "        beta        : Momentum hyperparameter\n",
        "    OUTPUTS:\n",
        "        v           : Velocity term\n",
        "    '''\n",
        "    v = beta * v_prev - lr * grad_tilde\n",
        "    return v"
      ],
      "metadata": {
        "id": "gBOecEVj5UT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Confidence bound-based active learning function'''\n",
        "def CB_learning(x, x_dim, m):\n",
        "    '''Active learning function based on expected improvement\n",
        "\n",
        "       This function selects a new sample from candidate pool to enrich the current training dataset.\n",
        "       The sample gets selected if it has the maximum expected improvement value.'''\n",
        "\n",
        "    kappa = 2.9\n",
        "\n",
        "    # 1-return the mean and variance of the prediction\n",
        "    mean, std = m.predict(np.array([x]))\n",
        "\n",
        "    # 2-Calculate the CB values\n",
        "    CB = mean - kappa * std\n",
        "\n",
        "    return CB"
      ],
      "metadata": {
        "id": "7Fdo4QlLS2RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CB_opt(x, x_dim, m):\n",
        "    CB = CB_learning(x, x_dim, m)\n",
        "    CB = CB.item()\n",
        "    return CB"
      ],
      "metadata": {
        "id": "Xj199kzxL8mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GP_inference(x,m):\n",
        "\t# Returns the mean and standard deviation of Gaussian process prediction\n",
        "\tmean,std = m.predict(np.array([x]))\n",
        "\treturn mean,std"
      ],
      "metadata": {
        "id": "0a9LJX6P0h5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GP_obj(x,m):\n",
        "  '''\n",
        "\tDescription:\n",
        "\tReturns the mean prediction of a Gaussian process at a point, x.\n",
        "\n",
        "\tInputs:\n",
        "\tx - input to be evaluated as either a list or 1-D numpy array\n",
        "\tm - model over which the mean is evaluated\n",
        "\n",
        "\tOutputs:\n",
        "\tmean - the mean of the Gaussian process posterior at x.\n",
        "\t'''\n",
        "  mean,std = GP_inference(x,m)\n",
        "  mean = mean.item()\n",
        "  std = std.item()\n",
        "  return mean"
      ],
      "metadata": {
        "id": "VmZrJdPguOzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BayesianOpt(f, N_x: int, bounds: List[Tuple[float]], BObudget: int = 100, n_s: int = 100) -> float:\n",
        "\n",
        "  T0=time.time()\n",
        "\n",
        "  x_dim = N_x\n",
        "\n",
        "  '''create initial point x_0'''\n",
        "  x_0 = []\n",
        "  for i in range(x_dim):\n",
        "    a = 0\n",
        "    x_0.append(a)\n",
        "\n",
        "  '''create training points for GP'''\n",
        "\n",
        "  x_train = [] # list for initial training points\n",
        "  y_train = []\n",
        "  x_real_list = [] # list for x value of all sample points\n",
        "  y_real_list = [] # list for y value of all sample points\n",
        "\n",
        "  bounds = np.array(bounds)\n",
        "  x_range = bounds[:,1] - bounds[:,0]\n",
        "\n",
        "  # sampling training points\n",
        "  sampler = qmc.LatinHypercube(d=x_dim)\n",
        "  sample = sampler.random(n=n_s)\n",
        "  l_bounds = bounds[:,0].tolist()\n",
        "  u_bounds = bounds[:,1].tolist()\n",
        "  x_train = qmc.scale(sample, l_bounds, u_bounds)\n",
        "  y_train = np.array([[f(xi)] for xi in x_train])\n",
        "\n",
        "  '''Main loop to update GP and find the most promising optimal value'''\n",
        "\n",
        "  n_iter = BObudget - n_s\n",
        "\n",
        "  for i in range(n_iter):\n",
        "    # train GPR model\n",
        "    kernel = GPy.kern.RBF(input_dim=x_dim, ARD=True)\n",
        "    m = GPy.models.GPRegression(x_train, y_train, kernel )\n",
        "    m.optimize_restarts(num_restarts = 30, verbose=False)\n",
        "\n",
        "    # find the next point according confidence bound strategy\n",
        "    CB_sample = minimize(CB_opt,x_0, args=(x_dim, m), method = 'Powell', bounds=bounds)\n",
        "    sample = CB_sample.x\n",
        "    sample_pred, sample_std = GP_inference(sample,m)\n",
        "\n",
        "    # avoid zero standard deviation sample point\n",
        "    if sample_std == 0:\n",
        "       sample = sample + np.random.uniform(-0.5,0.5)*((bounds[0,1]-bounds[0,0])/4)\n",
        "       # clip x to make it remain in the bound through symmetry\n",
        "       sample_clip = np.minimum(bounds[:,1], np.maximum(bounds[:,0], sample))\n",
        "       new_sample = 2 * sample_clip - sample\n",
        "    else:\n",
        "       new_sample = sample\n",
        "\n",
        "    # Calculate the true y value of the new sample\n",
        "    y_sample = f(new_sample)\n",
        "\n",
        "    print('new_sample = ', new_sample)\n",
        "    print('y_sample = ', y_sample)\n",
        "\n",
        "    # Enrich training dataset\n",
        "    x_train = np.vstack((x_train,new_sample))\n",
        "    y_train = np.vstack((y_train,y_sample))\n",
        "\n",
        "    # time limit\n",
        "    if time.time()-T0 >298:\n",
        "            break\n",
        "\n",
        "  best_index = np.argmin(y_train)\n",
        "  y_best = y_train[best_index]\n",
        "  x_best = x_train[best_index,:]\n",
        "  print('x_best', x_best)\n",
        "  print('y_best', y_best)\n",
        "\n",
        "  return x_best, y_best"
      ],
      "metadata": {
        "id": "f63b6fnTHqKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer(f, N_x: int, bounds: List[Tuple[float]], N: int = 100) -> float:\n",
        "  '''\n",
        "  Optimizer aims to optimize a black-box function 'f' using the dimensionality\n",
        "  'N_x', and box-'bounds' on the decision vector\n",
        "  Input:\n",
        "    f: function: taking as input a list of size N_x and outputing a float\n",
        "    N_x: int: number of dimensions\n",
        "    N: int: optional: Evaluation budget\n",
        "    bounds: List of size N where each element i is a tuple conisting of 2 floats\n",
        "            (lower, upper) serving as box-bounds on the ith element of x\n",
        "  Return:\n",
        "    lowest value found for f, f_min\n",
        "  '''\n",
        "\n",
        "  if N_x != len(bounds):\n",
        "    raise ValueError('Nbr of variables N_x does not match length of bounds')\n",
        "\n",
        "  ### Your code here\n",
        "\n",
        "  if N <= 50:\n",
        "     BObudget = 30\n",
        "     n_s = 20\n",
        "     x_best, y_best = BayesianOpt(f, N_x, bounds, BObudget, n_s)\n",
        "  else:\n",
        "     BObudget = 0\n",
        "     bounds = np.array(bounds)\n",
        "     x_best = np.add(bounds[:,1], np.add(bounds[:,0],0.1))/2\n",
        "\n",
        "  beta = 0.62\n",
        "  NAGbudget = N - BObudget\n",
        "  initial_x = x_best\n",
        "  y_best = NAG(f, bounds, beta, N_x, NAGbudget, initial_x)\n",
        "\n",
        "\n",
        "  return y_best\n"
      ],
      "metadata": {
        "id": "Yf378lQY8RYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although you are not allowed to use packaged solvers to directly optimize the functions in your implementation, you are still encouraged to compare your results against state-of-the-art DFO implementations, such as py-bobyqa, GPyOpt, Entmoot, or various methods within scipy.optimize.\n",
        "\n",
        "Note that minimize() requires slightly different inputs to our optimizer functions, such as an initial guess"
      ],
      "metadata": {
        "id": "EpF3ajGsUIyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_x = 2\n",
        "budget = 50\n",
        "bounds_RB = [(-2., 2.)]*N_x\n",
        "x0 = [0, 0]\n",
        "\n",
        "result_benchmark1 = minimize(\n",
        "    f_d2, x0,\n",
        "    bounds=bounds_RB,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "result_benchmark2 = minimize(\n",
        "    f_d2, x0,\n",
        "    bounds=bounds_RB,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "result_benchmark3 = minimize(\n",
        "    f_d2, x0,\n",
        "    bounds=bounds_RB,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "t0 = time.time()\n",
        "result_dummy = optimizer(f_d2, N_x, bounds_RB, budget)\n",
        "t = time.time()\n",
        "assert (t - t0 < 150), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "score = int(result_dummy < result_benchmark1.fun) + int(result_dummy < result_benchmark2.fun) + int(result_dummy < result_benchmark3.fun)\n",
        "\n",
        "print(f\"Nelder-Mead reached an optimum of {result_benchmark1.fun:.3f}\")\n",
        "print(f\"Powell reached an optimum of {result_benchmark2.fun:.3f}\")\n",
        "print(f\"COBYLA reached an optimum of {result_benchmark3.fun:.3f}\")\n",
        "\n",
        "print(f\"Your optimizer reached an optimum of {result_dummy:.3f}, beating {score}/3 benchmarks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JB8fhI_IP1U",
        "outputId": "47fc5b62-2457-4935-d367-fcb990898b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " /usr/local/lib/python3.8/dist-packages/scipy/optimize/_minimize.py:544: RuntimeWarning:Method COBYLA cannot handle bounds.\n",
            " /usr/local/lib/python3.8/dist-packages/GPy/kern/src/stationary.py:243: RuntimeWarning:invalid value encountered in true_divide\n",
            " /usr/local/lib/python3.8/dist-packages/GPy/kern/src/stationary.py:137: RuntimeWarning:overflow encountered in square\n",
            " /usr/local/lib/python3.8/dist-packages/GPy/kern/src/stationary.py:138: RuntimeWarning:invalid value encountered in add\n",
            " /usr/local/lib/python3.8/dist-packages/GPy/kern/src/rbf.py:76: RuntimeWarning:invalid value encountered in multiply\n",
            " /usr/local/lib/python3.8/dist-packages/GPy/kern/src/stationary.py:166: RuntimeWarning:overflow encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [-2.00000000e+00 -4.54217433e-04]\n",
            "y_sample =  1609.363390075178\n",
            "new_sample =  [-1.93033755 -1.99973557]\n",
            "y_sample =  3287.2242129832825\n",
            "new_sample =  [-1.18364583 -0.53763354]\n",
            "y_sample =  380.60507124439954\n",
            "new_sample =  [0.15946914 2.        ]\n",
            "y_sample =  390.5990004412579\n",
            "new_sample =  [-0.38610954  0.97190309]\n",
            "y_sample =  69.62498887620583\n",
            "new_sample =  [0.83303858 2.        ]\n",
            "y_sample =  170.6036804379903\n",
            "new_sample =  [-1.23389296 -0.815121  ]\n",
            "y_sample =  551.4336547917379\n",
            "new_sample =  [-1.37053116 -1.99973557]\n",
            "y_sample =  1509.5785803033261\n",
            "new_sample =  [0.64123211 0.4149118 ]\n",
            "y_sample =  0.13010805444209383\n",
            "new_sample =  [-0.93252103 -0.61268713]\n",
            "y_sample =  223.45080863569677\n",
            "x_best [0.64123211 0.4149118 ]\n",
            "y_best [0.13010805]\n",
            "Iterations:  20\n",
            "Optimal x :  [0.73271561 0.53378178]\n",
            "Minimal f :  0.07239599170346978\n",
            "Final grad:  [-0.15650273 -0.26263739]\n",
            "Nelder-Mead reached an optimum of 0.438\n",
            "Powell reached an optimum of 0.527\n",
            "COBYLA reached an optimum of 0.408\n",
            "Your optimizer reached an optimum of 0.072, beating 3/3 benchmarks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bfwpgyXLpo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training sets (30 marks)\n",
        "Two-dimensional datasets\n"
      ],
      "metadata": {
        "id": "HU0MTAMSX3aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Matyas(x: List[float]) -> float:\n",
        "  x = np.array(x)\n",
        "  return .26*(x[0]**2 + x[1]**2) - .48*x[0]*x[1]\n",
        "bounds_Matyas = [(-10., 7.), (-7., 10.)]\n",
        "\n",
        "def Bohachevsky(x: List[float]) -> float:\n",
        "  x = np.array(x)\n",
        "  out = x[0]**2+2*x[1]**2-.3*np.cos(3*np.pi*x[0])-.4*np.cos(4*np.pi*x[1]) + .7\n",
        "  return out\n",
        "bounds_Bohachevsky = [(-7., 10.), (-10., 7.)]\n",
        "\n",
        "train_2d_list = [f_d2, Matyas, Bohachevsky]\n",
        "bounds_train_2d = [bounds_RB, bounds_Matyas, bounds_Bohachevsky]\n",
        "budget_list = [50, 50, 50]\n",
        "dim_list = [2, 2, 2]"
      ],
      "metadata": {
        "id": "zlMgVw2JX9xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = {'benchmark1': [], 'benchmark2': [], 'benchmark3': [], 'dummy': [],}\n",
        "\n",
        "for iteration in zip(bounds_train_2d, train_2d_list, budget_list, dim_list):\n",
        "\n",
        "  bounds, function, budget, dim = iteration\n",
        "\n",
        "  benchmark1 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark2 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark3 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "  t0 = time.time()\n",
        "  dummy = optimizer(function, N_x, bounds, budget)\n",
        "  t = time.time()\n",
        "  assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "  all_results['benchmark1'] += [benchmark1.fun]\n",
        "  all_results['benchmark2'] += [benchmark2.fun]\n",
        "  all_results['benchmark3'] += [benchmark3.fun]\n",
        "  all_results['dummy'] += [dummy]\n",
        "\n",
        "\n",
        "print(f\"Optima reached {all_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zayi9AgPwYjZ",
        "outputId": "0fc1fe2e-0033-44ee-99f3-8323b9fb0f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [0.58249997 1.99993758]\n",
            "y_sample =  275.943958650518\n",
            "new_sample =  [-0.68905379  0.42976374]\n",
            "y_sample =  3.0556852820719715\n",
            "new_sample =  [0.27743093 0.07464792]\n",
            "y_sample =  0.5226442996797707\n",
            "new_sample =  [-1.99996566 -2.        ]\n",
            "y_sample =  3608.8349671008446\n",
            "new_sample =  [-1.99996566 -2.        ]\n",
            "y_sample =  3608.8349671008446\n",
            "new_sample =  [1.99993758 1.99993758]\n",
            "y_sample =  400.9249726874768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " /usr/local/lib/python3.8/dist-packages/GPy/kern/src/stationary.py:138: RuntimeWarning:overflow encountered in multiply\n",
            " /usr/local/lib/python3.8/dist-packages/GPy/kern/src/stationary.py:138: RuntimeWarning:overflow encountered in add\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [1.999899   1.99993758]\n",
            "y_sample =  400.86317901302715\n",
            "new_sample =  [-2.         -0.19962562]\n",
            "y_sample =  1772.6855268091817\n",
            "new_sample =  [1.99999996 1.2438537 ]\n",
            "y_sample =  760.6341475971664\n",
            "new_sample =  [1.58007453 0.49024849]\n",
            "y_sample =  402.89537421759525\n",
            "x_best [0.27743093 0.07464792]\n",
            "y_best [0.5226443]\n",
            "Iterations:  20\n",
            "Optimal x :  [0.55530948 0.30797092]\n",
            "Minimal f :  0.1977654743313061\n",
            "Final grad:  [-1.88012671  0.89074002]\n",
            "new_sample =  [0.01750009 0.01650355]\n",
            "y_sample =  1.1810752480820937e-05\n",
            "new_sample =  [-0.00744805 -0.01297696]\n",
            "y_sample =  1.1814024082722157e-05\n",
            "new_sample =  [0.00073974 0.00151859]\n",
            "y_sample =  2.026517571264689e-07\n",
            "new_sample =  [-0.00011083  0.00173595]\n",
            "y_sample =  8.790544514172362e-07\n",
            "new_sample =  [-2.83362713e-05  1.90281483e-04]\n",
            "y_sample =  1.221069314292257e-08\n",
            "new_sample =  [ 0.00333035 -0.00499213]\n",
            "y_sample =  1.734351012656867e-05\n",
            "new_sample =  [-3.80459966e-06  4.74199081e-03]\n",
            "y_sample =  5.855147601783365e-06\n",
            "new_sample =  [0.00497701 0.00342459]\n",
            "y_sample =  1.3083711987629166e-06\n",
            "new_sample =  [-0.00370922 -0.00558298]\n",
            "y_sample =  1.7412007791273708e-06\n",
            "new_sample =  [0.00431788 0.0020261 ]\n",
            "y_sample =  1.7155153206728588e-06\n",
            "x_best [-2.83362713e-05  1.90281483e-04]\n",
            "y_best [1.22106931e-08]\n",
            "Iterations:  2\n",
            "Optimal x :  [1.43497085e-04 7.95407719e-06]\n",
            "Minimal f :  4.822351277813777e-09\n",
            "Final grad:  [3.10934807e-06 3.10934807e-06]\n",
            "new_sample =  [0.0518021  0.00390625]\n",
            "y_sample =  0.038245505730044815\n",
            "new_sample =  [-0.07412476  0.04029921]\n",
            "y_sample =  0.12922571405613914\n",
            "new_sample =  [ 0.04780912 -0.00121821]\n",
            "y_sample =  0.03227850142429323\n",
            "new_sample =  [ 0.04716369 -0.00258824]\n",
            "y_sample =  0.03160257720840265\n",
            "new_sample =  [ 0.04686359 -0.00349021]\n",
            "y_sample =  0.03139463835016043\n",
            "new_sample =  [-0.05737614  0.0361038 ]\n",
            "y_sample =  0.0891695570411285\n",
            "new_sample =  [-0.02523589 -0.01289172]\n",
            "y_sample =  0.014652156084341561\n",
            "new_sample =  [-0.05782507  0.03699998]\n",
            "y_sample =  0.09200514760888767\n",
            "new_sample =  [-0.05599452  0.03743354]\n",
            "y_sample =  0.09019894998384703\n",
            "new_sample =  [-0.05482195  0.03796861]\n",
            "y_sample =  0.08972310751897372\n",
            "x_best [-0.02523589 -0.01289172]\n",
            "y_best [0.01465216]\n",
            "Iterations:  20\n",
            "Optimal x :  [ 1.51325640e-06 -9.89876658e-04]\n",
            "Minimal f :  3.290587047088955e-05\n",
            "Final grad:  [ 8.13603401e-06 -2.58751512e-02]\n",
            "Optima reached {'benchmark1': [0.43785398569338746, 0.0, 0.0], 'benchmark2': [0.5269010291621167, 0.0, 1.470590316188236e-11], 'benchmark3': [0.40767362067723956, 4.915240844238293e-09, 3.2671226013469834e-07], 'dummy': [0.1977654743313061, 4.822351277813777e-09, 3.290587047088955e-05]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing dimensionality unimodal"
      ],
      "metadata": {
        "id": "JX3RCzZa0KTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [2, 6, 10, 25]\n",
        "\n",
        "def Zakharov(d, x):\n",
        "    return np.sum([x[i]**2 for i in range(d)]) + np.sum([(0.5*i*x[i])**2 for i in range(d)]) + np.sum([(0.5*i*x[i])**4 for i in range(d)])\n",
        "\n",
        "Zakharov_list = [partial(Zakharov, dim) for dim in dims]\n",
        "bounds_Zakharov = [[(-5, 10)]*dim for dim in dims]\n",
        "print(bounds_Zakharov)\n",
        "budget_Zakharov = [50, 100, 100, 500, 1000, 1000]"
      ],
      "metadata": {
        "id": "0vXEAy-S0YC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f467a2-8e2c-49c7-bf02-7b81189a7a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(-5, 10), (-5, 10)], [(-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10)], [(-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10)], [(-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10), (-5, 10)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = {'benchmark1': [], 'benchmark2': [], 'benchmark3': [], 'dummy': [],}\n",
        "\n",
        "for iteration in zip(bounds_Zakharov, Zakharov_list, budget_Zakharov, dims):\n",
        "\n",
        "  bounds, function, budget, dim = iteration\n",
        "  x0 = [np.mean(list(b)) for b in bounds]\n",
        "\n",
        "  benchmark1 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark2 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark3 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "  t0 = time.time()\n",
        "  dummy = optimizer(function, dim, bounds, budget)\n",
        "  t = time.time()\n",
        "  # assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "  print('time =', t-t0)\n",
        "\n",
        "  all_results['benchmark1'] += [benchmark1.fun]\n",
        "  all_results['benchmark2'] += [benchmark2.fun]\n",
        "  all_results['benchmark3'] += [benchmark3.fun]\n",
        "  all_results['dummy'] += [dummy]\n",
        "\n",
        "print(f\"Optima reached {all_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzVG4rBS1O0f",
        "outputId": "2cb555ee-52b1-429b-800f-3fd477e5dddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [-4.99968275  2.03302892]\n",
            "y_sample =  31.231048163087394\n",
            "new_sample =  [9.99927514 5.02144527]\n",
            "y_sample =  171.24113274063245\n",
            "new_sample =  [ 9.99999996 -4.99999999]\n",
            "y_sample =  170.3124989311118\n",
            "new_sample =  [-0.6858968   0.95614581]\n",
            "y_sample =  1.6654597332867653\n",
            "new_sample =  [0.13852493 0.00588606]\n",
            "y_sample =  0.019232464685271875\n",
            "new_sample =  [ 9.99999998 -2.85185981]\n",
            "y_sample =  114.3005917264356\n",
            "new_sample =  [-1.92658629  6.90090063]\n",
            "y_sample =  204.98325855870448\n",
            "new_sample =  [ 1.79079069e-03 -1.76103707e-06]\n",
            "y_sample =  3.2069351564625285e-06\n",
            "new_sample =  [ 0.00042005 -0.00107325]\n",
            "y_sample =  1.6162776597598487e-06\n",
            "new_sample =  [ 0.00014971 -0.00205924]\n",
            "y_sample =  5.322977806074881e-06\n",
            "x_best [ 0.00042005 -0.00107325]\n",
            "y_best [1.61627766e-06]\n",
            "Iterations:  8\n",
            "Optimal x :  [-3.72747379e-06 -2.80479316e-04]\n",
            "Minimal f :  9.834970282651013e-08\n",
            "Final grad:  [-2.01123997e-05 -3.92432254e-05]\n",
            "time = 48.409862756729126\n",
            "Iterations:  99\n",
            "Optimal x :  [ 4.42169138e-12  4.35744569e-14  5.25371254e-16 -2.20201904e-20\n",
            " -1.60550073e-12 -7.50934410e-06]\n",
            "Minimal f :  4.0882930355227814e-10\n",
            "Final grad:  [ 1.27848981e-11  1.22891280e-13  2.79290480e-15 -3.46944695e-18\n",
            " -3.40774284e-12 -8.54291477e-05]\n",
            "time = 0.22878146171569824\n",
            "Iterations:  100\n",
            "Optimal x :  [-1.64888563e-07  2.23681993e-08  5.07409778e-12  3.23344687e-14\n",
            "  2.58271457e-14 -1.54659500e-17  1.18322429e-10  7.60786047e-05\n",
            "  1.21189366e-07  9.00736051e-14]\n",
            "Minimal f :  7.669067430485809e-08\n",
            "Final grad:  [-4.12298275e-07  7.29659377e-08 -1.03695719e-10 -1.88915550e-12\n",
            "  2.68673972e-13 -8.88178420e-16  6.26997121e-10  1.39758525e-03\n",
            "  4.47897763e-06  6.62048194e-12]\n",
            "time = 0.6411726474761963\n",
            "Iterations:  317\n",
            "Optimal x :  [-2.09621429e-05 -2.73972963e-07 -2.26755362e-13  1.16237347e-13\n",
            " -2.29407700e-19  3.15002535e-19 -7.55192678e-20  6.74135702e-20\n",
            " -4.27161047e-20 -8.66152585e-20 -7.41133933e-21  3.04928531e-20\n",
            "  6.02259524e-21 -3.09686181e-20 -3.81336096e-20 -9.38095849e-12\n",
            " -2.48286197e-10  1.02754103e-07  2.18445495e-08  5.02200992e-09\n",
            "  3.26300409e-14 -9.25252068e-13  4.41930536e-10  6.25942456e-10\n",
            "  2.81348559e-08]\n",
            "Minimal f :  4.404349517964722e-10\n",
            "Final grad:  [-4.31190934e-05 -7.09967486e-07 -9.64922586e-13  8.52810877e-13\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.07889403e-09\n",
            " -3.89523364e-08  2.13116430e-05  5.55947204e-06  2.80867242e-07\n",
            " -1.03355893e-10  9.42051943e-10 -1.43008911e-07  1.65019529e-08\n",
            "  1.23715151e-05]\n",
            "time = 4.090384006500244\n",
            "Optima reached {'benchmark1': [8.754702977709979e-05, 47.39961461942921, 4316.611991166451, 851104.4178509312], 'benchmark2': [4.999087443589626e-11, 1.4821880009456132e-09, 3.553586877415953e-09, 4.863624066143223e-08], 'benchmark3': [2.110543155784473e-08, 7.732896627020333e-05, 0.9473749646775761, 18.79384993626739], 'dummy': [9.834970282651013e-08, 4.0882930355227814e-10, 7.669067430485809e-08, 4.404349517964722e-10]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing dimensionality Rosenbrock"
      ],
      "metadata": {
        "id": "jWaCWgRj3bDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RB_decoupled(dim, x):\n",
        "  return np.sum([100*(x[i]**2 - x[i-1])**2+(x[i-1]-1)**2 for i in range(1,dim)])\n",
        "\n",
        "RB_list = [partial(RB_decoupled, dim) for dim in dims]\n",
        "bounds_RB = [[(-2, 2)]*dim for dim in dims]\n",
        "budget_RB = [50, 100, 100, 500, 1000, 1000]"
      ],
      "metadata": {
        "id": "A_6vvYYA3bzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = {'benchmark1': [], 'benchmark2': [], 'benchmark3': [], 'dummy': [],}\n",
        "\n",
        "for iteration in zip(bounds_RB, RB_list, budget_RB, dims):\n",
        "\n",
        "  bounds, function, budget, dim = iteration\n",
        "  x0 = [np.mean(list(b)) for b in bounds]\n",
        "\n",
        "  benchmark1 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark2 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark3 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "  t0 = time.time()\n",
        "  dummy = optimizer(function, dim, bounds, budget)\n",
        "  t = time.time()\n",
        "  assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "  all_results['benchmark1'] += [benchmark1.fun]\n",
        "  all_results['benchmark2'] += [benchmark2.fun]\n",
        "  all_results['benchmark3'] += [benchmark3.fun]\n",
        "  all_results['dummy'] += [dummy]\n",
        "\n",
        "print(f\"Optima reached {all_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seJY0_0F5Dyn",
        "outputId": "74d8c0a8-dc39-4283-b4fe-c8cf2ee5d9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [ 2.         -0.49249612]\n",
            "y_sample =  309.8621976967834\n",
            "new_sample =  [-1.99996566 -2.        ]\n",
            "y_sample =  3608.958580220184\n",
            "new_sample =  [-2.          1.99996566]\n",
            "y_sample =  3608.835173126837\n",
            "new_sample =  [-1.20611917  1.99989444]\n",
            "y_sample =  2714.795049279072\n",
            "new_sample =  [1.99973557 1.79900672]\n",
            "y_sample =  153.9395929479596\n",
            "new_sample =  [-0.01798285  1.44836995]\n",
            "y_sample =  448.67963444421974\n",
            "new_sample =  [ 1.99973557 -1.05023452]\n",
            "y_sample =  81.41427627464248\n",
            "new_sample =  [ 0.21199461 -0.74475507]\n",
            "y_sample =  12.362917353063562\n",
            "new_sample =  [-2.          1.04361058]\n",
            "y_sample =  963.2681195734003\n",
            "new_sample =  [-2.         -0.00859377]\n",
            "y_sample =  409.02954111407536\n",
            "x_best [0.25767181 0.4789633 ]\n",
            "y_best [0.63094766]\n",
            "Iterations:  20\n",
            "Optimal x :  [0.41029892 0.64010926]\n",
            "Minimal f :  0.34777861777341623\n",
            "Final grad:  [-1.74113071  0.6758331 ]\n",
            "Iterations:  100\n",
            "Optimal x :  [0.54178877 0.73646068 0.85823887 0.92643748 0.96183835 0.98064409]\n",
            "Minimal f :  0.30658352464609634\n",
            "Final grad:  [-1.23890684  0.62546511 -2.87510304  4.22829426 -4.78891436  2.21190988]\n",
            "Iterations:  100\n",
            "Optimal x :  [0.01000565 0.01033542 0.01570719 0.07509472 0.26026041 0.50658094\n",
            " 0.70980166 0.84367462 0.91609761 0.95873875]\n",
            "Minimal f :  4.731106101066865\n",
            "Final grad:  [-2.62081623e-04 -4.54175472e-03 -3.36033106e-03 -6.14447296e-01\n",
            " -1.43453252e+00 -1.60414368e+00 -4.35970664e-01 -1.60854179e+00\n",
            "  1.17997420e+00 -8.91305506e-01]\n",
            "Iterations:  500\n",
            "Optimal x :  [0.08609829 0.28439246 0.53019074 0.72694262 0.85210333 0.92284332\n",
            " 0.96056874 0.97996893 0.98999677 0.99484179 0.99757588 0.99857947\n",
            " 0.99952235 0.99949516 1.00002996 0.99971601 1.00015815 0.99978067\n",
            " 1.0001729  0.99982386 1.00014289 0.99987638 1.00008924 0.99994077\n",
            " 1.00002281]\n",
            "Minimal f :  1.6770609172506143\n",
            "Final grad:  [-0.78122523 -1.36735186 -1.29247302 -0.88378806 -0.50146678 -0.2929561\n",
            " -0.10305656 -0.14018907  0.0607986  -0.15361267  0.15981251 -0.20850399\n",
            "  0.22973746 -0.25829718  0.2721592  -0.28366123  0.28306779 -0.27724646\n",
            "  0.26049307 -0.23814404  0.20662901 -0.17032976  0.12764789 -0.08184756\n",
            "  0.03288347]\n",
            "Optima reached {'benchmark1': [0.9810644052413253, 4.951765305218043, 8.955098105408164, 23.94169979180296], 'benchmark2': [0.909089634980291, 4.9187859794798605, 8.878373476992214, 23.61107794394596], 'benchmark3': [0.7947092620402694, 4.9419615128824965, 8.909351211907389, 23.757162364866918], 'dummy': [0.34777861777341623, 0.30658352464609634, 4.731106101066865, 1.6770609172506143]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test sets (30 marks)"
      ],
      "metadata": {
        "id": "4ymJgJ-Qhr9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2-dimensional test functions (6 marks)\n",
        "\n",
        "def MCCormick(x: List[float]) -> float:\n",
        "  x = np.array(x)\n",
        "  return np.sin(x[0]+x[1]) + (x[0]-x[1])**2 - 1.5*x[0] + 2.5*x[1] + 1\n",
        "bounds_MCCormick = [(-1.5, 4.), (-3., 4.)]\n",
        "\n",
        "def THC(x: List[float]) -> float:\n",
        "  x = np.array(x)\n",
        "  out = 2*x[0]**2 - 1.05*x[1]**4 + x[0]**6/6 + x[0]*x[1] + x[1]**2\n",
        "  return out\n",
        "bounds_THC = [(-3., 2.), (-2., 3.)]\n",
        "\n",
        "def Branin(x: List[float]) -> float:\n",
        "  x = np.array(x)\n",
        "  a = 1 ; b = 5.1/(4*np.pi**2) ; c = 5/np.pi\n",
        "  r = 6 ; s = 10 ; t =1/(8*np.pi)\n",
        "  return a*(x[1] - b*x[0]**2 + c*x[0] - r)**2 + s*(1-t)*np.cos(x[0]) + s\n",
        "bounds_Branin = [(-5., 10.), (0., 15.)]\n",
        "\n",
        "test_2d_list = [MCCormick, THC, Branin]\n",
        "bounds_test_2d = [bounds_MCCormick, bounds_THC, bounds_Branin]\n",
        "budget_list = [50, 50, 50]\n",
        "dim_list = [2, 2, 2]"
      ],
      "metadata": {
        "id": "xolTzzOPXbov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = {'benchmark1': [], 'benchmark2': [], 'benchmark3': [], 'dummy': [],}\n",
        "\n",
        "for iteration in zip(bounds_test_2d, test_2d_list, budget_list, dim_list):\n",
        "\n",
        "  bounds, function, budget, dim = iteration\n",
        "  x0 = [np.mean(list(b)) for b in bounds]\n",
        "\n",
        "  benchmark1 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark2 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark3 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "  t0 = time.time()\n",
        "  dummy = optimizer(function, dim, bounds, budget)\n",
        "  t = time.time()\n",
        "  assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "  all_results['benchmark1'] += [benchmark1.fun]\n",
        "  all_results['benchmark2'] += [benchmark2.fun]\n",
        "  all_results['benchmark3'] += [benchmark3.fun]\n",
        "  all_results['dummy'] += [dummy]\n",
        "\n",
        "print(f\"Optima reached {all_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Du0MkNv7E2-",
        "outputId": "4e6667e7-3c3b-458c-f922-09c4791b1282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [-0.86408555 -1.80515957]\n",
            "y_sample =  -1.7861283085665098\n",
            "new_sample =  [-0.80264235 -1.82243502]\n",
            "y_sample =  -1.8059999575271917\n",
            "new_sample =  [-0.78612894 -1.80945474]\n",
            "y_sample =  -1.816528375059442\n",
            "new_sample =  [-0.62645766 -1.4563971 ]\n",
            "y_sample =  -1.8842445644355097\n",
            "new_sample =  [-0.55646817 -1.54876301]\n",
            "y_sample =  -1.9131128474867878\n",
            "new_sample =  [-0.5497539  -1.54778919]\n",
            "y_sample =  -1.9132148064329768\n",
            "new_sample =  [-0.54935698 -1.54768207]\n",
            "y_sample =  -1.9132171242505023\n",
            "new_sample =  [-0.54921702 -1.54761455]\n",
            "y_sample =  -1.9132178177296209\n",
            "new_sample =  [-0.54916417 -1.54758663]\n",
            "y_sample =  -1.91321806451358\n",
            "new_sample =  [-0.5491128 -1.547596 ]\n",
            "y_sample =  -1.9132183373194356\n",
            "x_best [-0.5491128 -1.547596 ]\n",
            "y_best [-1.91321834]\n",
            "Iterations:  16\n",
            "Optimal x :  [-0.54729087 -1.54710583]\n",
            "Minimal f :  -1.9132229207394715\n",
            "Final grad:  [-3.91006470e-05  3.65376472e-05]\n",
            "new_sample =  [0.33379761 0.31212496]\n",
            "y_sample =  0.4147151805625138\n",
            "new_sample =  [ 0.99298219 -0.15248099]\n",
            "y_sample =  2.0030700385333557\n",
            "new_sample =  [ 0.18991977 -0.85525981]\n",
            "y_sample =  0.07938568329455264\n",
            "new_sample =  [-3.  3.]\n",
            "y_sample =  54.450000350004565\n",
            "new_sample =  [-1.72859264  0.07083417]\n",
            "y_sample =  10.3049735740169\n",
            "new_sample =  [-0.14498349  3.        ]\n",
            "y_sample =  -76.44290849886497\n",
            "new_sample =  [-2.0239869   2.99966947]\n",
            "y_sample =  -62.43511889124978\n",
            "new_sample =  [-1.25117078  3.        ]\n",
            "y_sample =  -76.03329115912007\n",
            "new_sample =  [-0.64243075  2.99966947]\n",
            "y_sample =  -77.10443527620275\n",
            "new_sample =  [-0.60900097  3.        ]\n",
            "y_sample =  -77.12673584874166\n",
            "x_best [-0.60900097  3.        ]\n",
            "y_best [-77.12673585]\n",
            "Iterations:  4\n",
            "Optimal x :  [-3.38425216e+02  3.67211218e+23]\n",
            "Minimal f :  -1.9092071756510305e+94\n",
            "Final grad:  [0. 0.]\n",
            "new_sample =  [1.30497957e+00 6.14449051e-05]\n",
            "y_sample =  29.686871286269955\n",
            "new_sample =  [3.15891437 2.29283997]\n",
            "y_sample =  0.40030813719949343\n",
            "new_sample =  [3.14577242 2.29914126]\n",
            "y_sample =  0.3987219193521838\n",
            "new_sample =  [3.14030157 2.29498728]\n",
            "y_sample =  0.39825560849230257\n",
            "new_sample =  [3.14029552 2.28649332]\n",
            "y_sample =  0.398005298047968\n",
            "new_sample =  [3.14070786 2.28015299]\n",
            "y_sample =  0.3979110335028473\n",
            "new_sample =  [3.14099129 2.27776236]\n",
            "y_sample =  0.3978943533878887\n",
            "new_sample =  [3.14101627 2.27680384]\n",
            "y_sample =  0.3978907868650339\n",
            "new_sample =  [3.14110638 2.27676162]\n",
            "y_sample =  0.3978904039073665\n",
            "new_sample =  [3.14109348 2.2767495 ]\n",
            "y_sample =  0.3978904041360387\n",
            "x_best [3.14110638 2.27676162]\n",
            "y_best [0.3978904]\n",
            "Iterations:  20\n",
            "Optimal x :  [3.14157281 2.27499598]\n",
            "Minimal f :  0.39788736000069314\n",
            "Final grad:  [-2.37941742e-04 -4.16040421e-05]\n",
            "Optima reached {'benchmark1': [-1.9131749960062598, -51.383333333333326, 0.3979257454454661], 'benchmark2': [-1.9100804982933344, -77.15048930145369, 0.3978884070109103], 'benchmark3': [-1.9132229463005128, -2506951.253437187, 0.3978873996466721], 'dummy': [-1.9132229207394715, -1.9092071756510305e+94, 0.39788736000069314]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Increasing dimensionality: Well-behaved (12 marks)\n",
        "\n",
        "def Styblinski_Tang(dim, x):\n",
        "    d = dim\n",
        "    f = 1/2 * np.sum([(x[i]**4 - 16*x[i]**2 + 5*x[i]) for i in range(d)])\n",
        "    return f\n",
        "\n",
        "ST_list = [partial(Styblinski_Tang, dim) for dim in dims]\n",
        "bounds_ST = [[(-5., 5.)]*dim for dim in dims]\n",
        "budget_ST = [50, 100, 100, 500, 1000, 1000]\n"
      ],
      "metadata": {
        "id": "ashEGdUDDr2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = {'benchmark1': [], 'benchmark2': [], 'benchmark3': [], 'dummy': [],}\n",
        "\n",
        "for iteration in zip(bounds_ST, ST_list, budget_ST, dims):\n",
        "\n",
        "  bounds, function, budget, dim = iteration\n",
        "  x0 = [np.mean(list(b)) for b in bounds]\n",
        "\n",
        "  benchmark1 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark2 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark3 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "  t0 = time.time()\n",
        "  dummy = optimizer(function, dim, bounds, budget)\n",
        "  t = time.time()\n",
        "  assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "  all_results['benchmark1'] += [benchmark1.fun]\n",
        "  all_results['benchmark2'] += [benchmark2.fun]\n",
        "  all_results['benchmark3'] += [benchmark3.fun]\n",
        "  all_results['dummy'] += [dummy]\n",
        "\n",
        "print(f\"Optima reached {all_results}\")"
      ],
      "metadata": {
        "id": "1JhbrbjbEyIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a6c9afc-5b12-4dd9-f2f7-e2c29e8b5c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [ 0.033747   -2.61945649]\n",
            "y_sample =  -37.825352380096554\n",
            "new_sample =  [ 4.99994039 -1.0237944 ]\n",
            "y_sample =  114.59430641724492\n",
            "new_sample =  [0.85351387 4.99969584]\n",
            "y_sample =  121.51878146964899\n",
            "new_sample =  [-4.99994039  2.2863576 ]\n",
            "y_sample =  77.54947903069117\n",
            "new_sample =  [-2.20125472  2.27811455]\n",
            "y_sample =  -54.88389022579062\n",
            "new_sample =  [ 4.99999999 -3.44547346]\n",
            "y_sample =  91.87974822211706\n",
            "new_sample =  [ 3.23751009 -2.88524068]\n",
            "y_sample =  -59.98778883820047\n",
            "new_sample =  [-3.38352987  2.95932989]\n",
            "y_sample =  -58.8281976830356\n",
            "new_sample =  [ 2.28323117 -4.99905441]\n",
            "y_sample =  77.43303262632219\n",
            "new_sample =  [ 1.36040166 -0.70979321]\n",
            "y_sample =  -15.370029421173836\n",
            "x_best [-2.40903604 -2.55737292]\n",
            "y_best [-72.93804476]\n",
            "Iterations:  20\n",
            "Optimal x :  [-2.90105289 -2.90134441]\n",
            "Minimal f :  -78.33214220678047\n",
            "Final grad:  [-0.279459   -0.24749947]\n",
            "Iterations:  95\n",
            "Optimal x :  [-2.90353504 -2.90353504 -2.90353502 -2.90353508 -2.9035351  -2.90353498]\n",
            "Minimal f :  -234.9969942225217\n",
            "Final grad:  [-1.14440918e-05 -1.14440918e-05 -1.14440918e-05 -1.14440918e-05\n",
            " -1.52587891e-05 -1.33514404e-05]\n",
            "Iterations:  93\n",
            "Optimal x :  [-2.90353292 -2.90353292 -2.90353292 -2.90353292 -2.90353292 -2.90353292\n",
            " -2.90353292 -2.90353292 -2.9035331  -2.90353295]\n",
            "Minimal f :  -391.6616570375097\n",
            "Final grad:  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00 -3.81469727e-06]\n",
            "Iterations:  91\n",
            "Optimal x :  [-2.90353469 -2.90353469 -2.90353469 -2.90353469 -2.90353469 -2.90353469\n",
            " -2.90353469 -2.90353469 -2.90353469 -2.90353469 -2.90353469 -2.90353469\n",
            " -2.90353469 -2.90353469 -2.90353469 -2.90353469 -2.90353478 -2.90353478\n",
            " -2.90353478 -2.90353478 -2.90353478 -2.90353478 -2.90353478 -2.90353478\n",
            " -2.90353458]\n",
            "Minimal f :  -979.1541425940823\n",
            "Final grad:  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 7.62939453e-06]\n",
            "Optima reached {'benchmark1': [-39.67644905511944, -132.031428877364, -0.08419704172841083, -0.36922929629081386], 'benchmark2': [-78.33233140737026, -234.99699422211077, -391.66165703685124, -979.1541425921281], 'benchmark3': [-50.05889321248489, -150.1766768320466, -250.27807299769452, -625.736060334854], 'dummy': [-78.33214220678047, -234.9969942225217, -391.6616570375097, -979.1541425940823]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Increasing dimensionality: nonconvex (12 marks)\n",
        "\n",
        "def RB_aug(dim, x):\n",
        "  obj = np.sum([100*(x[i]**2 - x[i-1])**2+(x[i-1]-1)**2 for i in range(1,dim)])\n",
        "\n",
        "  penalty1 = np.sum([max(0,  (x[i-1] - 1)**3 - x[i] + 1)**2 for i in range(1,dim)])\n",
        "  penalty2 = np.sum([max(0,  x[0] + x[1] - 1.8)**2 for i in range(1,dim)])\n",
        "  return obj + 1000*(penalty1 + penalty2)\n",
        "\n",
        "RBaug_list = [partial(RB_decoupled, dim) for dim in dims]\n",
        "bounds_RBaug = [[(-2, 2)]*dim for dim in dims]\n",
        "budget_RBaug = [50, 100, 100, 500, 1000, 1000]"
      ],
      "metadata": {
        "id": "uDoIuOMqE5PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = {'benchmark1': [], 'benchmark2': [], 'benchmark3': [], 'dummy': [],}\n",
        "\n",
        "for iteration in zip(bounds_RBaug, RBaug_list, budget_RBaug, dims):\n",
        "\n",
        "  bounds, function, budget, dim = iteration\n",
        "  x0 = [np.mean(list(b)) for b in bounds]\n",
        "\n",
        "  benchmark1 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark2 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark3 = minimize(\n",
        "    function, x0,\n",
        "    bounds=bounds,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "  t0 = time.time()\n",
        "  dummy = optimizer(function, dim, bounds, budget)\n",
        "  t = time.time()\n",
        "  assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "  all_results['benchmark1'] += [benchmark1.fun]\n",
        "  all_results['benchmark2'] += [benchmark2.fun]\n",
        "  all_results['benchmark3'] += [benchmark3.fun]\n",
        "  all_results['dummy'] += [dummy]\n",
        "\n",
        "print(f\"Optima reached {all_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXT0oomRGXWB",
        "outputId": "73583887-1096-41f0-d118-cec91f642bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_sample =  [-2.          0.25738062]\n",
            "y_sample =  435.9367508336016\n",
            "new_sample =  [-1.99993746  0.66799822]\n",
            "y_sample =  607.3690533033723\n",
            "new_sample =  [ 2.         -1.74732684]\n",
            "y_sample =  111.91272415921469\n",
            "new_sample =  [-0.88802607 -0.20310843]\n",
            "y_sample =  89.92060762087692\n",
            "new_sample =  [1.99973557 1.9997013 ]\n",
            "y_sample =  400.6274370997357\n",
            "new_sample =  [ 0.05558274 -0.66718208]\n",
            "y_sample =  16.06678122987208\n",
            "new_sample =  [ 0.47091144 -0.66849321]\n",
            "y_sample =  0.33767047942785927\n",
            "new_sample =  [ 0.213014   -0.45657836]\n",
            "y_sample =  0.6214173941263297\n",
            "new_sample =  [2.        1.2528534]\n",
            "y_sample =  19.520830508136115\n",
            "new_sample =  [1.26498928 1.12427703]\n",
            "y_sample =  0.0703174187698884\n",
            "x_best [1.26498928 1.12427703]\n",
            "y_best [0.07031742]\n",
            "Iterations:  20\n",
            "Optimal x :  [1.15776059 1.07632665]\n",
            "Minimal f :  0.024940022986035208\n",
            "Final grad:  [-0.21987565  1.16123184]\n",
            "Iterations:  100\n",
            "Optimal x :  [0.54178877 0.73646068 0.85823887 0.92643748 0.96183835 0.98064409]\n",
            "Minimal f :  0.30658352464609634\n",
            "Final grad:  [-1.23890684  0.62546511 -2.87510304  4.22829426 -4.78891436  2.21190988]\n",
            "Iterations:  100\n",
            "Optimal x :  [0.01000565 0.01033542 0.01570719 0.07509472 0.26026041 0.50658094\n",
            " 0.70980166 0.84367462 0.91609761 0.95873875]\n",
            "Minimal f :  4.731106101066865\n",
            "Final grad:  [-2.62081623e-04 -4.54175472e-03 -3.36033106e-03 -6.14447296e-01\n",
            " -1.43453252e+00 -1.60414368e+00 -4.35970664e-01 -1.60854179e+00\n",
            "  1.17997420e+00 -8.91305506e-01]\n",
            "Iterations:  500\n",
            "Optimal x :  [0.08609829 0.28439246 0.53019074 0.72694262 0.85210333 0.92284332\n",
            " 0.96056874 0.97996893 0.98999677 0.99484179 0.99757588 0.99857947\n",
            " 0.99952235 0.99949516 1.00002996 0.99971601 1.00015815 0.99978067\n",
            " 1.0001729  0.99982386 1.00014289 0.99987638 1.00008924 0.99994077\n",
            " 1.00002281]\n",
            "Minimal f :  1.6770609172506143\n",
            "Final grad:  [-0.78122523 -1.36735186 -1.29247302 -0.88378806 -0.50146678 -0.2929561\n",
            " -0.10305656 -0.14018907  0.0607986  -0.15361267  0.15981251 -0.20850399\n",
            "  0.22973746 -0.25829718  0.2721592  -0.28366123  0.28306779 -0.27724646\n",
            "  0.26049307 -0.23814404  0.20662901 -0.17032976  0.12764789 -0.08184756\n",
            "  0.03288347]\n",
            "Optima reached {'benchmark1': [0.9810644052413253, 4.951765305218043, 8.955098105408164, 23.94169979180296], 'benchmark2': [0.909089634980291, 4.9187859794798605, 8.878373476992214, 23.61107794394596], 'benchmark3': [0.7947092620402694, 4.9419615128824965, 8.909351211907389, 23.757162364866918], 'dummy': [0.024940022986035208, 0.30658352464609634, 4.731106101066865, 1.6770609172506143]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test sets (30 marks)"
      ],
      "metadata": {
        "id": "N8r39AFwtckb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supplementary problem (40 marks)"
      ],
      "metadata": {
        "id": "hW7OAinhtsLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose *ONLY ONE* of the two supplementary problems (A *OR* B) for the remaining marks.\n",
        "You will build on your solution to the above problem (or build another solution from scratch) by considering one of the following extensions to your algorithm:\n",
        "\n",
        "- Discrete variables\n",
        "- High-dimensional problems with low effective dimensionality\n",
        "\n"
      ],
      "metadata": {
        "id": "Cawfh6-tG3Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A) Discrete variables (40 marks)\n",
        "\n",
        "Like in conventional optimization, the presence of discrete decision variables complicates DFO significantly because they prevent the use of local information that many DFO methods rely on. If you choose this supplementary problem, you will build on your previous algorithm (or construct a new one) that takes as additional input a dictionary of the integer variable keys and a list of possible values they can take on, and returns the best possible solution found in the given evaluation budget.\n",
        "\n",
        "You may find the below dummy optimizer and test function useful for illustration:"
      ],
      "metadata": {
        "id": "PVXQe6s1WE9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Discrete test function (5 marks)\n",
        "def f_discrete(x: List[float], y: Dict[str, int]):\n",
        "  np.random.seed(0)\n",
        "  keys = list(y.keys())\n",
        "  continuous = np.sum([x_**2 for x_ in x])\n",
        "  discrete = np.sum([np.random.normal()*y[key] for key in keys])\n",
        "  bilinear = np.random.choice(x)*y[random.choice(keys)]\n",
        "  return continuous + discrete + bilinear\n",
        "\n",
        "bounds = [(-2, 4), (-4, 2)] ; N_x = 2\n",
        "integers_dict = {'y1': [-1, 0, 1], 'y2': [-1, 1],}"
      ],
      "metadata": {
        "id": "onZeYkpNanqh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer_discrete(f, N_x: int,\n",
        "                       bounds: List[Tuple[float]],\n",
        "                       integers: Dict[str, List[int]],\n",
        "                       N: int = 100) -> float:\n",
        "  '''\n",
        "  Optimizer aims to optimize a black-box function 'f' using the dimensionality\n",
        "  'N_x', and box-'bounds' on the decision vector\n",
        "  Input:\n",
        "    f: function: taking as input a list of size N_x and outputing a float\n",
        "    N_x: int: number of CONTINUOUS dimensions\n",
        "    N: int: optional: Evaluation budget\n",
        "    bounds: List of size N where each element i is a tuple conisting of 2 floats\n",
        "            (lower, upper) serving as box-bounds on the ith element of x\n",
        "    integers:\n",
        "  Return:\n",
        "    lowest value found for f, f_min\n",
        "  '''\n",
        "  if N_x != len(bounds):\n",
        "    raise ValueError('Nbr of variables N_x does not match length of bounds')\n",
        "\n",
        "  ### Your code here\n",
        "  x = [np.mean(bounds[i]) for i in range(N_x)]\n",
        "  keys = integers.keys()\n",
        "  y = {key: np.random.choice(integers[key]) for key in keys}\n",
        "  f_best = f(x, y)\n",
        "  ###\n",
        "\n",
        "  return f_best\n",
        "\n",
        "t0 = time.time()\n",
        "test_result = optimizer_discrete(\n",
        "    f_discrete, len(bounds), bounds, integers_dict, N=100,\n",
        ")\n",
        "t = time.time()"
      ],
      "metadata": {
        "id": "dVsKXBsuWDxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Other form of discrete test function\n",
        "keys = list(integers_dict.keys())\n",
        "bounds_discr = [(-2, 4), (-4, 2), (-1, 1), (-1, 1)]\n",
        "x0_discr = np.array([np.mean(list(b_)) for b_ in bounds_discr])\n",
        "\n",
        "def f_discr_cont(x_extended):\n",
        "  ## this function is the same as f_discrete() but where the discrete variables\n",
        "  ## are relaxed to be continuous so you can check against\n",
        "  ## continuous scipy.optimize.minimize() DFO solvers\n",
        "  np.random.seed(0)\n",
        "  continuous = np.sum([x_**2 for x_ in x_extended[:2]])\n",
        "  discrete = np.sum([np.random.normal()*x_ for x_ in x_extended[2:]])\n",
        "  x_ = np.random.choice(x_extended[:2])\n",
        "  idx = keys.index(random.choice(keys))\n",
        "  bilinear = x_*x_extended[2+idx]\n",
        "  return continuous + discrete + bilinear"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gm_XX5SyYgib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "budget = 10000\n",
        "benchmark = minimize(\n",
        "    f_discr_cont, x0_discr,\n",
        "    bounds=bounds_discr,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "x_opt = benchmark.x.copy()\n",
        "x_opt[2:] = np.round(x_opt[2:])\n",
        "actual_f = f_discrete(x_opt[:2], {'y1': x_opt[-2], 'y2': x_opt[-1]})\n",
        "print(f\"After rounding of the discrete variables, Powell reaches an optimum of {actual_f:.3f} at {x_opt}\")\n",
        "\n",
        "t0 = time.time()\n",
        "result_dummy = optimizer_discrete(f_discrete, N_x, bounds, integers_dict, budget)\n",
        "t = time.time()\n",
        "assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "print(f\"Your optimizer reaches an optimum of {result_dummy:.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AYQHu_Pfq_n",
        "outputId": "3330df17-c84c-4946-b129-3a5398b05ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After rounding of the discrete variables, Powell reaches an optimum of -2.396 at [ 0.09622247  0.40375334 -1.         -1.        ]\n",
            "Your optimizer reaches an optimum of 2.400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using discrete derivative-free optimization solvers for hyperparameter tuning"
      ],
      "metadata": {
        "id": "6Yf4nbLyhrde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with the wine regression playground for MLPClassifier hyperparameter tuning"
      ],
      "metadata": {
        "id": "dNd8B5x_sWuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
        "from sklearn.model_selection import (GridSearchCV, StratifiedShuffleSplit, cross_validate)"
      ],
      "metadata": {
        "id": "WrodrqitiQW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Wine classification example (10 marks)\n",
        " # ============= Wine classification =============\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/dsrscientist/DSData/master/winequality-red.csv\")\n",
        "X = df.values[:,:-1]\n",
        "y = df.values[:,-1]\n",
        "cv_N = 5\n",
        "\n",
        "def wine_classification(continous, discrete):\n",
        "  pipe_all = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      MLPClassifier(\n",
        "          hidden_layer_sizes=(discrete['hidden1'], discrete['hidden2']),\n",
        "          learning_rate_init=continuous[0],\n",
        "          random_state=0,\n",
        "          early_stopping=bool(discrete['early_stopping']),\n",
        "          ),\n",
        "      )\n",
        "  cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "  scores = cross_validate(pipe_all, X, y, cv=cv, return_train_score=True)\n",
        "  return np.mean(scores['test_score'])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S9TVNe4peuPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search"
      ],
      "metadata": {
        "id": "uoetlVOprfPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      MLPClassifier(\n",
        "          hidden_layer_sizes=(100, 50),\n",
        "          learning_rate_init=1,\n",
        "          random_state=0,\n",
        "          early_stopping=True,\n",
        "          ),\n",
        "      )\n",
        "\n",
        "hidden_layers1 = [80, 100, 120]\n",
        "hidden_layers2 = [40, 50, 60]\n",
        "learning_rates = np.logspace(-1, 1, 6)\n",
        "param_grid = {\n",
        "    'mlpclassifier__hidden_layer_sizes': list(itertools.product(hidden_layers1, hidden_layers2)),\n",
        "    'mlpclassifier__learning_rate_init': learning_rates,\n",
        "}\n",
        "\n",
        "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "grid = GridSearchCV(pipe, param_grid=param_grid, cv=cv)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\n",
        "    \"The best parameters are %s with a score of %0.2f\"\n",
        "    % (grid.best_params_, grid.best_score_)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dwASTLrjmBl",
        "outputId": "df960205-aafc-43ed-aacb-cad7d1632b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameters are {'mlpclassifier__hidden_layer_sizes': (120, 40), 'mlpclassifier__learning_rate_init': 0.1} with a score of 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "continuous = [grid.best_params_['mlpclassifier__learning_rate_init']]\n",
        "discrete = {\n",
        "    'hidden1': grid.best_params_['mlpclassifier__hidden_layer_sizes'][0],\n",
        "    'hidden2': grid.best_params_['mlpclassifier__hidden_layer_sizes'][1],\n",
        "    'early_stopping': 1,\n",
        "    }\n",
        "\n",
        "gridsearch_score = wine_classification(continuous, discrete)\n",
        "print(f\"Gridsearch reached a score of {gridsearch_score:.3f}\")\n",
        "\n",
        "bounds = [(0.1, 10)] ; N_x = 1 ; budget = 100\n",
        "integers_dict = {\n",
        "    'hidden1': hidden_layers1,\n",
        "    'hidden2': hidden_layers2,\n",
        "    'early_stopping': [0,1],\n",
        "    }\n",
        "\n",
        "t0 = time.time()\n",
        "dummy = optimizer_discrete(wine_classification, N_x, bounds, integers_dict, budget)\n",
        "t = time.time()\n",
        "assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "print(f\"Your optimizer reached an optimum of {dummy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PojaZ1JPoAa7",
        "outputId": "d476c5db-d0b2-489f-b141-e11e8fd921a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gridsearch reached a score of 0.621\n",
            "Your optimizer reached an optimum of 0.582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisiting ML models that make sense. Doing extensive GridSearchCV is too expensive on this problem"
      ],
      "metadata": {
        "id": "65ZDUC36sUrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load VLE data\n",
        "directory_2 = \"https://raw.githubusercontent.com/OptiMaL-PSE-Lab/ML-course/master/VLE%20prediction/Data.xlsx\"\n",
        "directory_1 = \"https://raw.githubusercontent.com/OptiMaL-PSE-Lab/ML-course/master/VLE%20prediction/Test-Dataset%20(no%20noise).xlsx\"\n",
        "df_test = pd.read_excel(directory_1, sheet_name='Phase Envelope')\n",
        "df = pd.read_excel(directory_2, sheet_name='Phase Envelope')\n",
        "df_array = df.values\n",
        "df_test_array = df_test.values\n",
        "X_idx = [0]\n",
        "yP_idx = 1\n",
        "yliq_idx = 2\n",
        "yvap_idx = 3\n",
        "X_train = df_array[:, X_idx]\n",
        "X_test = df_test_array[:, X_idx]\n",
        "y_liq_train = df_array[:, yliq_idx].squeeze()\n",
        "y_liq_test = df_test_array[:, yliq_idx].squeeze()\n",
        "y_vap_train = df_array[:, yvap_idx].squeeze()\n",
        "y_vap_test = df_test_array[:, yvap_idx].squeeze()\n",
        "yP_train = df_array[:, yP_idx].squeeze()\n",
        "yP_test = df_test_array[:, yP_idx].squeeze()\n",
        "\n",
        "df_critical = pd.read_excel(directory_1, sheet_name='Critical Point')\n",
        "T_c = df_critical['Temperature / K'][0]\n",
        "P_c = df_critical['Pressure / Pa'][0]\n",
        "rho_c = df_critical['Density / (mol m^-3)'][0]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gvqLBNfas85h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DFO for hyperparameter tuning (15 marks)\n",
        "def physical_ML(continuous, discrete, print_corr=True, print_pred=True):\n",
        "\n",
        "  pipe_liq = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      MLPRegressor(\n",
        "          hidden_layer_sizes=(discrete['liq_hidden1'],discrete['liq_hidden2']),\n",
        "          learning_rate_init=continuous[0],\n",
        "          random_state=0,\n",
        "          )\n",
        "      )\n",
        "\n",
        "  pipe_vap = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      MLPRegressor(\n",
        "          hidden_layer_sizes=(discrete['vap_hidden'],),\n",
        "          learning_rate_init=continuous[1],\n",
        "          random_state=0,\n",
        "          )\n",
        "      )\n",
        "\n",
        "  pipe_P = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      MLPRegressor(\n",
        "          hidden_layer_sizes=(discrete['P_hidden1'],discrete['P_hidden2']),\n",
        "          learning_rate_init=continuous[2],\n",
        "          random_state=0,\n",
        "          )\n",
        "      )\n",
        "\n",
        "  pipe_liq.fit(X_train, y_liq_train)\n",
        "  pipe_vap.fit(X_train, y_vap_train)\n",
        "  pipe_P.fit(X_train, yP_train)\n",
        "\n",
        "  R2_train_liq = pipe_liq.score(X_train, y_liq_train)\n",
        "  R2_train_vap = pipe_vap.score(X_train, y_vap_train)\n",
        "  R2_test_liq = pipe_liq.score(X_test, y_liq_test)\n",
        "  R2_test_vap = pipe_vap.score(X_test, y_vap_test)\n",
        "  R2_train_T = pipe_P.score(X_train, yP_train)\n",
        "  R2_test_T = pipe_P.score(X_test, yP_test)\n",
        "\n",
        "  if print_corr:\n",
        "    print(f\"R squared value on liq test set: {R2_test_liq:.3f}\")\n",
        "    print(f\"R squared value on vap test set: {R2_test_vap:.3f}\")\n",
        "    print(f\"R squared value on T test set: {R2_test_T:.3f}\")\n",
        "\n",
        "  XP = np.array([ [P] for P in range(int(T_c/2), int(T_c*2))])\n",
        "\n",
        "  rho_liq_pred = pipe_liq.predict(XP)\n",
        "  rho_vap_pred = pipe_vap.predict(XP)\n",
        "  P_pred = pipe_P.predict(XP)\n",
        "  rho_deviation = np.abs(rho_liq_pred - rho_vap_pred)\n",
        "  idx = np.where(rho_deviation == np.min(rho_deviation))\n",
        "\n",
        "  if print_pred:\n",
        "    print(f\"Predicted critical point: T - {float(XP[idx])} ; P - {float(P_pred[idx]):.0f} ; rho_liq - {float(rho_liq_pred[idx]):.0f} ; rho_vap - {float(rho_vap_pred[idx]):.0f}\")\n",
        "    print(f\"Real critical point: T {T_c} - P - {P_c:.0f} ; rho_liq - {rho_c:.0f} ; rho_vap - {rho_c:.0f}\")\n",
        "    print(\"\")\n",
        "  min_corr = -R2_test_liq-R2_test_vap-R2_test_T # negative because high R2 the better\n",
        "  predictions = (float(XP[idx])-T_c)**2/T_c**2 + (float(P_pred[idx])-P_c)**2/P_c**2\n",
        "  predictions += ((float(rho_liq_pred[idx]) - rho_c)**2 + (float(rho_vap_pred[idx]) - rho_c)**2)/rho_c**2\n",
        "\n",
        "  return min_corr + predictions"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9fja_z5ptUP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_x = 3\n",
        "bounds = [(0.1, 10)]*N_x;\n",
        "hidden_layers1 = [60, 80, 100, 120, 140]\n",
        "hidden_layers2 = [1, 10, 20, 40]\n",
        "integers_dict = {\n",
        "    'liq_hidden1': hidden_layers1,\n",
        "    'liq_hidden2': hidden_layers2,\n",
        "    'P_hidden1': hidden_layers1,\n",
        "    'P_hidden2': hidden_layers2,\n",
        "    'vap_hidden': hidden_layers1,\n",
        "    }\n",
        "\n",
        "budget = 100\n",
        "\n",
        "t0 = time.time()\n",
        "dummy = optimizer_discrete(physical_ML, N_x, bounds, integers_dict, budget)\n",
        "t = time.time()\n",
        "assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "print(f\"Your optimizer reached an optimum of {dummy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL7PgrtIvrLF",
        "outputId": "1a52c5dc-79bd-4a51-e4aa-fc6b7b6370fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R squared value on liq test set: -16.657\n",
            "R squared value on vap test set: 0.918\n",
            "R squared value on T test set: 0.793\n",
            "Predicted critical point: T - 272.0 ; P - 4728667 ; rho_liq - 2174 ; rho_vap - 2047\n",
            "Real critical point: T 304.17 - P - 7383113 ; rho_liq - 9596 ; rho_vap - 9596\n",
            "\n",
            "Your optimizer reached an optimum of 16.303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test functions (10 marks)"
      ],
      "metadata": {
        "id": "LiEY2dVf2hQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1QiE5q-r2Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B) Reduced-order optimization (40 marks)\n",
        "\n",
        "As you might have already noticed, DFO excels in low dimensions ($n_d$), where there are only few degrees of freedom in the decision variables that map the objective through an expensive black-box. As a rule of thumb, DFO solvers are useful up to around 100 dimensions. This is especially problematic for model-based derivative-free optimization solvers where typically at least $\\mathcal{O}(n_d)$ are needed to construct meaningful surrogates for simple linear surrogates.\n",
        "\n",
        "However, many problems present particular mathematical structures that can be exploited to address the curse of dimensionality. These problems present a true / intrinsic / effective dimensionality that is less than the 'natural' dimensionality $n_e \\leq n_d$. These problems arise in hyperparameter estimation for example, where changes in only a few dimensions explains much of the variability in the objective. These functions are also said to have 'low effective dimensionality', or 'active subspaces'. An example in chemical engineering might be the following: Imagine you want to use derivative-free optimization to (among other variables) find the optimal set of flow of pure component A ($F_A$) and B ($F_B$) into a reactor. In this case, although you theoretically have complete freedom over both flows, in practice, you could optimize the ratio of A to B instead.\n",
        "\n",
        "The following [paper](https://academic.oup.com/imaiai/article/11/1/167/6278168) has a nice introduction and motivating case. There is no need to read through the theoretical derivations.\n",
        "\n",
        "An alternative to the iterative random embeddings presented in the previous approach would be to use partial least squares or other dimensionality reduction technique to find the lower-order embeddings.\n",
        "\n",
        "Another approach would be to use statistical methods (either after an initial space-filling design, or iteratively) to decide which dimensions are most important and worth 'fine-tuning'. This framework of considering a subset (or linear combinations) of  the orginal dimensions could also be employed to direct DFO methods (for example coordinate search). [[Other example]](https://link.springer.com/article/10.1007/s11590-016-1028-2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0_baDZJorqCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rosenbrock_() - x can have any arbitrary dimension $n_d$, but only the first 2 matter ($n_e = 2$)\n",
        "\n",
        "rosenbrock_higher() - takes x with $n_d$ dimensions and constructs $n_d$ new dimensions as a linear combination of the original $n_d$ while preserving the effective dimensionality of $n_e = 2$"
      ],
      "metadata": {
        "id": "IRlF4kLU5G7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ortho_group"
      ],
      "metadata": {
        "id": "76NRtP_vQE43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title High-dimensional Rosenbrock with low effective dimensionality (16 marks)\n",
        "def rosenbrock_(x, seed=0):\n",
        "    x = np.array(x).squeeze()\n",
        "    assert x.ndim == 1\n",
        "    return 100*(x[1]-x[0]**2)**2 + (x[0]-1)**2\n",
        "\n",
        "def rosenbrock_higher(Q, x):\n",
        "    # Q is used to 'mix up' the fake and effective dimensions\n",
        "    x = np.array(x).squeeze()\n",
        "    assert x.ndim == 1\n",
        "    dim = len(x)\n",
        "    # np.random.seed(seed)\n",
        "    # Q = ortho_group.rvs(len(x))\n",
        "    x = x.reshape(-1,1)\n",
        "\n",
        "    new_x = (Q @ x).squeeze()\n",
        "    return rosenbrock_(new_x)"
      ],
      "metadata": {
        "id": "kfnR0K7p5Q5R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct an optimizer that in addition to the optimizer() inputs also receives $n_e$ and leverages this knowledge to perform sample-efficient reduced-order optimization"
      ],
      "metadata": {
        "id": "QffFIGm76A1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer_reduced(f, N_x: int, bounds: List[Tuple[float]], n_e, N: int = 100) -> float:\n",
        "  '''\n",
        "  Optimizer aims to optimize a black-box function 'f' using the dimensionality\n",
        "  'N_x', and box-'bounds' on the decision vector\n",
        "  Input:\n",
        "    f: function: taking as input a list of size N_x and outputing a float\n",
        "    N_x: int: number of dimensions\n",
        "    n_e: int: number of effective/active/true dimensions, usually << n_e\n",
        "    N: int: optional: Evaluation budget\n",
        "    bounds: List of size N where each element i is a tuple conisting of 2 floats\n",
        "            (lower, upper) serving as box-bounds on the ith element of x\n",
        "  Return:\n",
        "    lowest value found for f, f_min\n",
        "  '''\n",
        "  if N_x != len(bounds):\n",
        "    raise ValueError('Nbr of variables N_x does not match length of bounds')\n",
        "\n",
        "  ### Your code here\n",
        "  x = [np.mean(bounds[i]) for i in range(N_x)]\n",
        "  f_best = f(x)\n",
        "  ###\n",
        "\n",
        "  return f_best"
      ],
      "metadata": {
        "id": "L73vs4X-fsew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training benchmarks on increasing levels of Rosenbrock"
      ],
      "metadata": {
        "id": "-LzPih_zQH4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [3, 5, 10, 20, 50, 100, 200, 400]\n",
        "budgets = [50, 100, 100, 200, 500, 1000, 2000, 4000]\n",
        "\n",
        "all_results = {'benchmark1': [], 'benchmark2': [], 'benchmark3': [], 'dummy': [],}\n",
        "\n",
        "N_e = 2\n",
        "\n",
        "for budget, N_dim in zip(budgets, dims):\n",
        "\n",
        "  print(f\"{budget} evaluations for {N_dim} dimensions\")\n",
        "\n",
        "  np.random.seed(0)\n",
        "  Q = ortho_group.rvs(N_dim)\n",
        "  wrapper_RB = partial(rosenbrock_higher, Q)\n",
        "\n",
        "  x0 = np.array([0]*N_dim)\n",
        "  bounds = [(-.5, .5)]*N_dim\n",
        "\n",
        "  benchmark1 = minimize(\n",
        "    wrapper_RB, x0,\n",
        "    bounds=bounds,\n",
        "    method='Nelder-Mead',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark2 = minimize(\n",
        "    wrapper_RB, x0,\n",
        "    bounds=bounds,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "\n",
        "  benchmark3 = minimize(\n",
        "    wrapper_RB, x0,\n",
        "    bounds=bounds,\n",
        "    method='COBYLA',\n",
        "    options={'maxiter': budget},\n",
        "    )\n",
        "\n",
        "  t0 = time.time()\n",
        "  dummy = optimizer_reduced(wrapper_RB, N_dim, bounds, N_e, budget)\n",
        "  t = time.time()\n",
        "  assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "\n",
        "  all_results['benchmark1'] += [benchmark1.fun]\n",
        "  all_results['benchmark2'] += [benchmark2.fun]\n",
        "  all_results['benchmark3'] += [benchmark3.fun]\n",
        "  all_results['dummy'] += [dummy]\n",
        "\n",
        "print(f\"Optima reached {all_results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceVZ1ZX2bm9u",
        "outputId": "bde90876-cedb-4b14-95a7-cd89527902bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 evaluations for 3 dimensions\n",
            "100 evaluations for 5 dimensions\n",
            "100 evaluations for 10 dimensions\n",
            "200 evaluations for 20 dimensions\n",
            "500 evaluations for 50 dimensions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py:545: RuntimeWarning: Method COBYLA cannot handle bounds.\n",
            "  RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 evaluations for 100 dimensions\n",
            "2000 evaluations for 200 dimensions\n",
            "4000 evaluations for 400 dimensions\n",
            "Optima reached {'benchmark1': [0.7893822376755726, 0.6532304531584366, 0.9090875086165523, 0.9780590509232587, 0.9951491302804742, 0.9976153406653555, 0.9985613957359483, 0.9990726294010398], 'benchmark2': [0.4463858580365516, 0.29352168617463, 0.26251068649162607, 0.16242376816983514, 0.07773778283758705, 0.005552736739907255, 2.9466241542255258e-05, 1.1902632184615152e-13], 'benchmark3': [0.286536988093545, 0.250675852293827, 0.3643126419916998, 0.07004675739374235, 0.04490218582897468, 8.968359627701061e-09, 4.2940679819355594e-13, 2.1302723184459655e-14], 'dummy': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's optimize a 376-dimensional derivative-free optimization problem to solve a supply chain planning problem"
      ],
      "metadata": {
        "id": "BsUYC3utTcpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simulation data\n",
        "\n",
        "products = [\"PA\", \"PB\", \"PC\", \"PD\", \"TEE\", \"TGE\"]\n",
        "secondary_sites = [\"Asia\", \"America\"]\n",
        "resources = [\"Granulates\", \"Compress\", \"Coat\", \"QC\", \"Packing\"]\n",
        "# intermediates = ['TA', 'TB', 'I1', 'I2', 'I3', 'I4']\n",
        "T_set = np.arange(1, 25).tolist()\n",
        "\n",
        "\n",
        "# F = {(\"PA\", t): (t - 1) * 4000 / 100 + 5000 for t in T_set}\n",
        "# F_ = {(\"PC\", t): (t - 1) * 1e4 / 100 + 10000 for t in T_set}\n",
        "# F__ = {(\"TEE\", t): (t - 1) * 1e5 / 100 + 400000 for t in T_set}\n",
        "F = {(\"PA\", t): 5000*(1+.1*int(t>=5) - .2*int(t>=10) + .3*int(t>=15) - .4*int(t>=20)) for t in T_set}\n",
        "F_ = {(\"PC\", t): 10000*(1+.1*int(t>=5) - .2*int(t>=10) + .3*int(t>=15) - .4*int(t>=20)) for t in T_set}\n",
        "F__ = {(\"TEE\", t): 300000*(1+.1*int(t>=5) - .2*int(t>=10) + .3*int(t>=15) - .4*int(t>=20)) for t in T_set}\n",
        "F_TGE = {(\"TGE\", t): 300000*(1-.1*int(t>=5) + .2*int(t>=10) - .3*int(t>=15) + .4*int(t>=20)) for t in T_set}\n",
        "F_PB = {(\"PB\", t): 5000*(1-.1*int(t>=5) + .2*int(t>=10) - .3*int(t>=15) + .4*int(t>=20)) for t in T_set}\n",
        "F_PD = {(\"PD\", t): 10000*(1-.1*int(t>=5) + .2*int(t>=10) - .3*int(t>=15) + .4*int(t>=20)) for t in T_set}\n",
        "F.update(F_) ; F.update(F__) ; F.update(F_TGE)\n",
        "F.update(F_PB) ; F.update(F_PD)\n",
        "\n",
        "data = { None: {\n",
        "        \"P\": {None: products},\n",
        "        \"L\": {None: secondary_sites},\n",
        "        \"R\": {None: resources},\n",
        "        \"T\": {None: T_set},\n",
        "        # Initial storage\n",
        "        \"S0\": {\n",
        "            \"PA\": 15e3, \"PB\": 15e3,\n",
        "            \"PC\": 6e4, \"PD\": 6e4,\n",
        "            \"TEE\": 2e6, \"TGE\": 2e6},\n",
        "        \"SAIP0\": {None: 3000},\n",
        "        \"SI0\": {None: 3000},\n",
        "        \"SAIS0\": {\"Asia\": 480, \"America\": 360},\n",
        "        #Safety storage\n",
        "        \"IAIPstar0\": {None: 3000},\n",
        "        \"IIstar0\": {None: 3000},\n",
        "        \"Istar0\": {\n",
        "            \"PA\": 15e3, \"PB\": 15e3, \"PC\": 6e4, \"PD\": 6e4,\n",
        "            \"TEE\": 2e6, \"TGE\": 2e6,\n",
        "            },\n",
        "        \"IAISstar0\": {\"Asia\": 400, \"America\": 300},\n",
        "        #Costs\n",
        "        \"CT\": {\"Asia\": 15, \"America\": 10}, #Transport\n",
        "        \"CS\": {\"PA\": 0.1, \"PC\": 0.15, \"TEE\": 0.1,\n",
        "                \"PB\": 0.09, \"PD\": 0.16, \"TGE\": 0.1,}, # Storage\n",
        "        \"CS_SAIS\": {\"Asia\": 0.02, \"America\": 0.03},\n",
        "        # 'CS_SAIS': {'Asia': 0, 'America': 0},\n",
        "        \"CS_AIP\": {None: 0.02},\n",
        "        \"CS_I\": {None: 0.01},\n",
        "        # 'CS_AIP': {None: 0}, 'CS_I': {None: 0},\n",
        "        \"RM_Cost\": {None: 0.1}, #Raw material cost\n",
        "        \"AI_Cost\": {None: 0.5}, # AI cost (should not matter for the whole company)\n",
        "        # 'RM_Cost': {None: 0}, 'AI_Cost': {None: 0},\n",
        "        \"Price\": {\"PA\": 10, \"PB\": 10, \"PC\": 10, \"PD\": 10,\n",
        "                  \"TEE\": 1, \"TGE\": 1,},\n",
        "        \"CP\": {\"PA\": 1.2, \"PC\": 1.2, \"TEE\": 0.06,\n",
        "               \"PB\": 1.19, \"PD\": 1.21, \"TGE\": 0.06,}, # Production\n",
        "        \"CP_I\": {None: 0.05e-1},\n",
        "        \"CP_AI\": {None: 0.2e-1},\n",
        "        \"SP_AI\": {None: 2500}, # Selling price\n",
        "        # 'SP_AI': {None: 0},\n",
        "        \"LT\": {\"Asia\": 4, \"America\": 3}, # lead time\n",
        "        \"Q\": {\"PA\": 20e-6 * 12 / 0.02, \"PC\": 6e-4 / 0.02, \"TEE\": 3e-5,\n",
        "              \"PB\": 21e-6 * 12 / 0.02, \"PD\": 5.9e-4 / 0.02, \"TGE\": 3.1e-5,}, # AI to material conversion\n",
        "        # No FPAI(T), FPTA(T), FPTB(T), FPI4(T), FPI3(T), FPI2(T), FPI1(T), FTP(T,K)\n",
        "        \"A\": {  # Resource availability\n",
        "            (\"Asia\", \"Granulates\"): 120*2,\n",
        "            (\"Asia\", \"Compress\"): 480*2,\n",
        "            (\"Asia\", \"Coat\"): 480*2,\n",
        "            (\"Asia\", \"QC\"): 1800*2,\n",
        "            (\"Asia\", \"Packing\"): 320*2,\n",
        "            (\"America\", \"Granulates\"): 120*2,\n",
        "            (\"America\", \"Compress\"): 120*2,\n",
        "            (\"America\", \"Coat\"): 120*2,\n",
        "            (\"America\", \"QC\"): 720*2,\n",
        "            (\"America\", \"Packing\"): 160*2,\n",
        "        },\n",
        "        \"U\": { # Resource time utilised per product\n",
        "            (\"Asia\", \"Granulates\"): 133e-3,\n",
        "            (\"Asia\", \"Compress\"): 350e-3,\n",
        "            (\"Asia\", \"Coat\"): 333e-3,\n",
        "            (\"Asia\", \"QC\"): 2,\n",
        "            (\"Asia\", \"Packing\"): 267e-3,\n",
        "            (\"America\", \"Granulates\"): 133e-3,\n",
        "            (\"America\", \"Compress\"): 350e-3,\n",
        "            (\"America\", \"Coat\"): 333e-3,\n",
        "            (\"America\", \"QC\"): 2,\n",
        "            (\"America\", \"Packing\"): 267e-3,\n",
        "        },\n",
        "        \"CCH\": {\n",
        "            (\"PA\", \"PA\"): 0,\n",
        "            (\"PA\", \"PB\"): 3e4,\n",
        "            (\"PA\", \"PC\"): 1e8,\n",
        "            (\"PA\", \"PD\"): 1e8,\n",
        "            (\"PA\", \"TEE\"): 2.9e4,\n",
        "            (\"PA\", \"TGE\"): 2.9e4,\n",
        "            (\"PB\", \"PA\"): 3e4,\n",
        "            (\"PB\", \"PB\"): 0,\n",
        "            (\"PB\", \"PC\"): 1e8,\n",
        "            (\"PB\", \"PD\"): 1e8,\n",
        "            (\"PB\", \"TEE\"):2.9e4,\n",
        "            (\"PB\", \"TGE\"):2.9e4,\n",
        "            (\"PC\", \"PA\"): 1e8,\n",
        "            (\"PC\", \"PB\"): 1e8,\n",
        "            (\"PC\", \"PC\"): 0,\n",
        "            (\"PC\", \"PD\"): 1e4,\n",
        "            (\"PC\", \"TEE\"):1e8,\n",
        "            (\"PC\", \"TGE\"):1e8,\n",
        "            (\"PD\", \"PA\"): 1e8,\n",
        "            (\"PD\", \"PB\"): 1e8,\n",
        "            (\"PD\", \"PC\"): 1e4,\n",
        "            (\"PD\", \"PD\"): 0,\n",
        "            (\"PD\", \"TEE\"): 1e8,\n",
        "            (\"PD\", \"TGE\"): 1e8,\n",
        "            (\"TEE\", \"PA\"): 3.1e4,\n",
        "            (\"TEE\", \"PB\"): 3.1e4,\n",
        "            (\"TEE\", \"PC\"): 1e8,\n",
        "            (\"TEE\", \"PD\"): 1e8,\n",
        "            (\"TEE\", \"TEE\"): 0,\n",
        "            (\"TEE\", \"TGE\"): 3e4,\n",
        "            (\"TGE\", \"PA\"): 3.1e4,\n",
        "            (\"TGE\", \"PB\"): 3.1e4,\n",
        "            (\"TGE\", \"PC\"): 1e8,\n",
        "            (\"TGE\", \"PD\"): 1e8,\n",
        "            (\"TGE\", \"TEE\"): 3e4,\n",
        "            (\"TGE\", \"TGE\"): 0,\n",
        "        },\n",
        "        \"X\": { # matches between final products and secondary sites\n",
        "            (\"Asia\", \"PA\"): 1,\n",
        "            (\"Asia\", \"PC\"): 0,\n",
        "            (\"Asia\", \"TEE\"): 1,\n",
        "            (\"Asia\", \"PB\"): 1,\n",
        "            (\"Asia\", \"PD\"): 0,\n",
        "            (\"Asia\", \"TGE\"): 1,\n",
        "            (\"America\", \"PA\"): 0,\n",
        "            (\"America\", \"PC\"): 1,\n",
        "            (\"America\", \"TEE\"): 0,\n",
        "            (\"America\", \"PB\"): 0,\n",
        "            (\"America\", \"PD\"): 1,\n",
        "            (\"America\", \"TGE\"): 0,\n",
        "        },\n",
        "        \"F\": F, # Demand forecast\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pZmPPRyoO7sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simulation\n",
        "def state_to_control_t(t, N_t, T_set):\n",
        "    dummy_array = np.arange(1, 1+N_t)\n",
        "    N_total = len(T_set)\n",
        "    T_min = T_set[0]\n",
        "    idx = 1 + (t - T_min) // int(N_total/N_t)\n",
        "    return min(idx, N_t)\n",
        "\n",
        "def simulate(Production, TP, Forecast, Sales, data, seed=0, random=True):\n",
        "\n",
        "    N_tc = data[None]['N_t'][None]\n",
        "    T_set = data[None][\"T\"][None]\n",
        "    to_Tc = lambda t: state_to_control_t(t, N_tc, T_set)\n",
        "\n",
        "    Storage = {}\n",
        "    Demand = {}\n",
        "    N_t = len(T_set)\n",
        "    P_set = data[None][\"P\"][None]\n",
        "\n",
        "    Demand[\"PA\"] = np.zeros(N_t)\n",
        "    Demand[\"PC\"] = np.zeros(N_t)\n",
        "    Demand[\"TEE\"] = np.zeros(N_t)\n",
        "    Demand[\"PB\"] = np.zeros(N_t)\n",
        "    Demand[\"PD\"] = np.zeros(N_t)\n",
        "    Demand[\"TGE\"] = np.zeros(N_t)\n",
        "\n",
        "    for t in T_set:\n",
        "        for p in P_set:\n",
        "            if random:\n",
        "                Demand[p][t - 1] = np.random.uniform(\n",
        "                    0.8 * Forecast[p][t - 1], 1.2 * Forecast[p][t - 1]\n",
        "                )\n",
        "            else:\n",
        "                Demand[p][t - 1] = Forecast[p][t - 1]\n",
        "\n",
        "    Storage[\"PA\"] = np.zeros(N_t)\n",
        "    Storage[\"PC\"] = np.zeros(N_t)\n",
        "    Storage[\"TEE\"] = np.zeros(N_t)\n",
        "    Storage[\"PB\"] = np.zeros(N_t)\n",
        "    Storage[\"PD\"] = np.zeros(N_t)\n",
        "    Storage[\"TGE\"] = np.zeros(N_t)\n",
        "    Storage[\"AIP\"] = np.zeros(N_t)\n",
        "    Storage[\"AIS_Am\"] = np.zeros(N_t)\n",
        "    Storage[\"AIS_As\"] = np.zeros(N_t)\n",
        "    Storage[\"I\"] = np.zeros(N_t)\n",
        "\n",
        "    Dummy_Sales = {}\n",
        "    Dummy_Sales[\"PA\"] = np.zeros(N_t)\n",
        "    Dummy_Sales[\"PB\"] = np.zeros(N_t)\n",
        "    Dummy_Sales[\"PC\"] = np.zeros(N_t)\n",
        "    Dummy_Sales[\"PD\"] = np.zeros(N_t)\n",
        "    Dummy_Sales[\"TEE\"] = np.zeros(N_t)\n",
        "    Dummy_Sales[\"TGE\"] = np.zeros(N_t)\n",
        "\n",
        "    for p in P_set:\n",
        "        Storage[p][0] = max(0,data[None][\"S0\"][p] - min(Sales[p][0], Demand[p][0]) + Production[p][0])\n",
        "        Dummy_Sales[p][0] =  min(Sales[p][0], Demand[p][0], data[None][\"S0\"][p]+Production[p][0])\n",
        "    Storage[\"AIS_Am\"][0] = max(0,\n",
        "        data[None][\"SAIS0\"][\"America\"]\n",
        "        - 1.1 * (Production[\"PC\"][0] * data[None][\"Q\"][\"PC\"] + Production[\"PD\"][0] * data[None][\"Q\"][\"PD\"])\n",
        "    )\n",
        "    Storage[\"AIS_As\"][0] = max(0,\n",
        "        data[None][\"SAIS0\"][\"Asia\"]\n",
        "        - 1.1\n",
        "        * (\n",
        "            Production[\"PA\"][0] * data[None][\"Q\"][\"PA\"]\n",
        "                    + Production[\"TEE\"][0] * data[None][\"Q\"][\"TEE\"] +\n",
        "                    Production[\"PB\"][0] * data[None][\"Q\"][\"PB\"]\n",
        "                    + Production[\"TGE\"][0] * data[None][\"Q\"][\"TGE\"]\n",
        "        ) # + TP[\"Asia\"][1]\n",
        "    )\n",
        "    Storage[\"AIP\"][0] = max(0,\n",
        "        data[None][\"SAIP0\"][None] + Production[\"AI\"][0] - (TP[\"America\"][0] + TP[\"Asia\"][0])\n",
        "    )\n",
        "    Storage[\"I\"][0] = max(0,\n",
        "        data[None][\"SI0\"][None] + Production[\"I\"][0] - 1.1 * Production[\"AI\"][0]\n",
        "    )\n",
        "\n",
        "    for t in T_set:\n",
        "        for p in P_set:\n",
        "            if t - 1 > 0:\n",
        "                Storage[p][t - 1] = max(0,\n",
        "                    Storage[p][t - 2] - min(Sales[p][t-1], Demand[p][t - 1]) + Production[p][t - 1]\n",
        "                )\n",
        "                Dummy_Sales[p][t-1] =  min(Sales[p][t-1], Demand[p][t-1], Storage[p][t - 2] +Production[p][0])\n",
        "        t_Am = data[None][\"LT\"][\"America\"]\n",
        "        t_As = data[None][\"LT\"][\"Asia\"]\n",
        "        if t - 1 - t_Am >= 0:\n",
        "            Storage[\"AIS_Am\"][t - 1] = max(0,\n",
        "                Storage[\"AIS_Am\"][t - 2]\n",
        "                + TP[\"America\"][to_Tc(t-t_Am)-1]\n",
        "                - 1.1 * (Production[\"PC\"][t - 1] * data[None][\"Q\"][\"PC\"] + Production[\"PD\"][t - 1] * data[None][\"Q\"][\"PD\"])\n",
        "            )\n",
        "        elif t - 1 > 0:\n",
        "            Storage[\"AIS_Am\"][t - 1] = max(0,\n",
        "                Storage[\"AIS_Am\"][t - 2] # + TP[\"America\"]\n",
        "                - 1.1 * (Production[\"PC\"][t - 1] * data[None][\"Q\"][\"PC\"] + Production[\"PD\"][t - 1] * data[None][\"Q\"][\"PD\"])\n",
        "            )\n",
        "        if t - 1 - t_As >= 0:\n",
        "            Storage[\"AIS_As\"][t - 1] = max(0,\n",
        "                Storage[\"AIS_As\"][t - 2]\n",
        "                + TP[\"Asia\"][to_Tc(t-t_As)-1]\n",
        "                - 1.1\n",
        "                * (\n",
        "                    Production[\"PA\"][t - 1] * data[None][\"Q\"][\"PA\"]\n",
        "                    + Production[\"TEE\"][t - 1] * data[None][\"Q\"][\"TEE\"] +\n",
        "                    Production[\"PB\"][t - 1] * data[None][\"Q\"][\"PB\"]\n",
        "                    + Production[\"TGE\"][t - 1] * data[None][\"Q\"][\"TGE\"]\n",
        "                )\n",
        "            )\n",
        "        elif t - 1 > 0:\n",
        "            Storage[\"AIS_As\"][t - 1] = max(0,\n",
        "                Storage[\"AIS_As\"][t - 2] # + TP[\"Asia\"]\n",
        "                - 1.1\n",
        "                * (\n",
        "                    Production[\"PA\"][t - 1] * data[None][\"Q\"][\"PA\"]\n",
        "                    + Production[\"TEE\"][t - 1] * data[None][\"Q\"][\"TEE\"] +\n",
        "                    Production[\"PB\"][t - 1] * data[None][\"Q\"][\"PB\"]\n",
        "                    + Production[\"TGE\"][t - 1] * data[None][\"Q\"][\"TGE\"]\n",
        "                )\n",
        "            )\n",
        "        if t - 1 > 0:\n",
        "            Storage[\"AIP\"][t - 1] = max(0,\n",
        "                Storage[\"AIP\"][t - 2]\n",
        "                + Production[\"AI\"][t - 1]\n",
        "                - (TP[\"America\"][to_Tc(t)-1] + TP[\"Asia\"][to_Tc(t)-1])\n",
        "            )\n",
        "            Storage[\"I\"][t - 1] = max(0,\n",
        "                Storage[\"I\"][t - 2]\n",
        "                + Production[\"I\"][t - 1]\n",
        "                - 1.1 * Production[\"AI\"][t - 1]\n",
        "            )\n",
        "    return Storage, Demand, Dummy_Sales"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O4NS1AieUQYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DFO wrapper for simulation (14 marks)\n",
        "def get_Forecast(data):\n",
        "\n",
        "    T_set = data[None][\"T\"][None]\n",
        "    N_t = len(T_set)\n",
        "    F_PA = np.zeros(N_t)\n",
        "    F_PB = np.zeros(N_t)\n",
        "    F_PC = np.zeros(N_t)\n",
        "    F_PD = np.zeros(N_t)\n",
        "    F_TEE = np.zeros(N_t)\n",
        "    F_TGE = np.zeros(N_t)\n",
        "    for t in T_set:\n",
        "        F_PA[t-1] = data[None]['F']['PA',t]\n",
        "        F_PB[t-1] = data[None]['F']['PB',t]\n",
        "        F_PC[t-1] = data[None]['F']['PC',t]\n",
        "        F_PD[t-1] = data[None]['F']['PD',t]\n",
        "        F_TEE[t-1] = data[None]['F']['TEE',t]\n",
        "        F_TGE[t-1] = data[None]['F']['TGE',t]\n",
        "\n",
        "    Forecast = {}\n",
        "    Forecast['PA'] = F_PA\n",
        "    Forecast['PC'] = F_PC\n",
        "    Forecast['TEE'] = F_TEE\n",
        "    Forecast['PB'] = F_PB\n",
        "    Forecast['PD'] = F_PD\n",
        "    Forecast['TGE'] = F_TGE\n",
        "\n",
        "    return Forecast\n",
        "\n",
        "\n",
        "def get_cost(input_data, Production, TP, Sales, penalization=1):\n",
        "\n",
        "    T_set = input_data[None][\"T\"][None]\n",
        "    N_t = len(T_set)\n",
        "    N_Tc = input_data[None][\"N_t\"][None]\n",
        "\n",
        "    to_Tc = lambda t: state_to_control_t(t, N_Tc, T_set)\n",
        "\n",
        "    Forecast = get_Forecast(input_data)\n",
        "\n",
        "    Storage, Demand, Dummy_Sales = simulate(Production, TP, Forecast, Sales, input_data, random=False)\n",
        "\n",
        "    P_set = input_data[None]['P'][None]\n",
        "    R_set = input_data[None]['R'][None]\n",
        "    T_set = input_data[None]['T'][None]\n",
        "    L_set = input_data[None]['L'][None]\n",
        "\n",
        "    Price = input_data[None]['Price']\n",
        "    CP = input_data[None]['CP']\n",
        "    CS = input_data[None]['CS']\n",
        "    CS_I = input_data[None]['CS_I'][None]\n",
        "    CS_SAIS = input_data[None]['CS_SAIS']\n",
        "    CS_AIP = input_data[None]['CS_AIP'][None]\n",
        "    CT = input_data[None]['CT']\n",
        "    CP_I = input_data[None]['CP_I'][None]\n",
        "    CP_AI = input_data[None]['CP_AI'][None]\n",
        "\n",
        "    U = input_data[None]['U']\n",
        "    X = input_data[None]['X']\n",
        "    A = input_data[None]['A']\n",
        "    Q = input_data[None]['Q']\n",
        "    IIstar0 = input_data[None]['IIstar0'][None]\n",
        "    Istar0 = input_data[None]['Istar0']\n",
        "    IAISstar0 = input_data[None]['IAISstar0']\n",
        "    IAIPstar0 = input_data[None]['IAIPstar0'][None]\n",
        "\n",
        "    prod_cost = sum(\n",
        "        CP[p]*Production[p][t-1] for p in P_set for t in T_set\n",
        "    ) + sum(\n",
        "        CP_I*Production['I'][t-1] for t in T_set\n",
        "    ) + sum(\n",
        "        CP_AI*Production['AI'][t-1] for t in T_set\n",
        "    )\n",
        "    transp_cost = sum(CT[l]*TP[l][to_Tc(t)-1] for l in L_set for t in T_set)\n",
        "    store_cost = np.sum(\n",
        "        [CS[p] * Storage[p][t-1] for p in P_set for t in T_set]\n",
        "    ) + np.sum(\n",
        "        [CS_SAIS['Asia'] * Storage['AIS_As'][t-1] for t in T_set]\n",
        "    ) + np.sum(\n",
        "        [CS_SAIS['America'] * Storage['AIS_Am'][t-1] for t in T_set]\n",
        "    ) + np.sum(\n",
        "        [CS_AIP * Storage['AIP'][t-1] for t in T_set]\n",
        "    ) + np.sum(\n",
        "        [CS_I * Storage['I'][t-1] for t in T_set]\n",
        "    )\n",
        "\n",
        "    for p in P_set:\n",
        "        for t in T_set:\n",
        "            assert Storage[p][t-1] >= -1e-5\n",
        "\n",
        "    sales = np.sum([Price[p] * Dummy_Sales[p][t-1] for p in P_set for t in T_set])\n",
        "\n",
        "    res_viol = sum(\n",
        "        max(0, U[l, r]* sum(\n",
        "                Production[p][t-1]*X[l, p]*Q[p] for p in P_set\n",
        "            )/A[l, r] - 1\n",
        "        )**2 for l in L_set for r in R_set for t in T_set\n",
        "    )\n",
        "\n",
        "    prod_UL = sum(\n",
        "        max(\n",
        "            0, Production[p][t-1]/500e3-1\n",
        "        )**2 for p in P_set for t in T_set\n",
        "    ) + sum(\n",
        "        max(\n",
        "            0, Production['AI'][t-1]/500e3-1\n",
        "        )**2 for t in T_set\n",
        "    ) + sum(\n",
        "        max(\n",
        "            0, Production['I'][t-1]/500e3-1\n",
        "        )**2 for t in T_set\n",
        "    )\n",
        "\n",
        "    istar_constr = 0\n",
        "    for t in T_set:\n",
        "        if t > 4:\n",
        "            istar_constr += sum(max(- Storage[p][t-1]/Istar0[p]+1, 0)**2 for p in P_set)\n",
        "            istar_constr += max(- Storage['I'][t-1]/IIstar0+1, 0)**2\n",
        "            istar_constr += max(- Storage['AIS_Am'][t-1]/IAISstar0[\"America\"]+1, 0)**2\n",
        "            istar_constr += max(- Storage['AIS_As'][t-1]/IAISstar0[\"Asia\"]+1, 0)**2\n",
        "            istar_constr += max(- Storage['AIP'][t-1]/IAIPstar0+1, 0)**2\n",
        "        else:\n",
        "            istar_constr += sum(max(- Storage[p][t-1]/Istar0[p]+1/4, 0)**2 for p in P_set)\n",
        "            istar_constr += max(- Storage['I'][t-1]/IIstar0+1/4, 0)**2\n",
        "            istar_constr += max(- Storage['AIS_Am'][t-1]/IAISstar0[\"America\"]+1/4, 0)**2\n",
        "            istar_constr += max(- Storage['AIS_As'][t-1]/IAISstar0[\"Asia\"]+1/4, 0)**2\n",
        "            istar_constr += max(- Storage['AIP'][t-1]/IAIPstar0+1/4, 0)**2\n",
        "\n",
        "    ### Add safeS, safeSI, safeSAIS, safeSAIP\n",
        "\n",
        "    penalty = istar_constr + prod_UL + res_viol\n",
        "\n",
        "    return (prod_cost + transp_cost + store_cost - sales)/1e6 + penalty*penalization\n",
        "\n",
        "def wrapper(x, input_data, penalty=1):\n",
        "    T_set = input_data[None][\"T\"][None]\n",
        "    N_t = len(T_set)\n",
        "    N_Tc = input_data[None][\"N_t\"][None]\n",
        "\n",
        "    to_Tc = lambda t: state_to_control_t(t, N_Tc, T_set)\n",
        "\n",
        "    P_PA = x[:N_t]\n",
        "    P_PB = x[N_t:2*N_t]\n",
        "    P_TEE = x[2*N_t:3*N_t]\n",
        "    P_TGE = x[3*N_t:4*N_t]\n",
        "    P_PC = x[4*N_t:5*N_t]\n",
        "    P_PD = x[5*N_t:6*N_t]\n",
        "\n",
        "    SA_PA =  x[6*N_t:7*N_t]\n",
        "    SA_PB =  x[7*N_t:8*N_t]\n",
        "    SA_TEE = x[8*N_t:9*N_t]\n",
        "    SA_TGE = x[9*N_t:10*N_t]\n",
        "    SA_PC =  x[10*N_t:11*N_t]\n",
        "    SA_PD =  x[11*N_t:12*N_t]\n",
        "\n",
        "    PI = x[12*N_t:13*N_t]\n",
        "    PAI = x[13*N_t:14*N_t]\n",
        "\n",
        "    TP_As = x[14*N_t:14*N_t+1*N_Tc]\n",
        "    TP_Am = x[14*N_t+1*N_Tc:14*N_t+2*N_Tc]\n",
        "\n",
        "    Production = {} ; TP = {} ; Sales = {}\n",
        "    Production['PA'] = P_PA\n",
        "    Production['PC'] = P_PC\n",
        "    Production['TEE'] = P_TEE\n",
        "    Production['PB'] = P_PB\n",
        "    Production['PD'] = P_PD\n",
        "    Production['TGE'] = P_TGE\n",
        "    Production['AI'] = PAI\n",
        "    Production['I'] = PI\n",
        "    Sales['PA'] =  SA_PA\n",
        "    Sales['PC'] =  SA_PC\n",
        "    Sales['TEE'] = SA_TEE\n",
        "    Sales['PB'] =  SA_PB\n",
        "    Sales['PD'] =  SA_PD\n",
        "    Sales['TGE'] = SA_TGE\n",
        "    TP['Asia'] = TP_As\n",
        "    TP['America'] = TP_Am\n",
        "\n",
        "    return get_cost(input_data, Production, TP, Sales, penalization=penalty)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FAPdofzuTweu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best possible achievable optimum of this problem is: $f_{opt} \\approx -9.43$"
      ],
      "metadata": {
        "id": "WDNOVUw8s68g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rho = 100\n",
        "Nt = 5\n",
        "\n",
        "data_copy = data.copy()\n",
        "data_copy[None].update({'N_t': {None: Nt}, 'Tc': {None: np.arange(1, 1+Nt)}})\n",
        "\n",
        "dfo_f = lambda x: wrapper(x, data_copy, rho)\n",
        "\n",
        "b = []\n",
        "b += [(0, 20000)]*48 + [(0, 500000)]*48 + [(0, 20000)]*24 + [(0, 30000)]*24 + [(0, 10000)]*48 + [(0, 4000000)]*48\n",
        "b += [(0, 15000)]*48 + [(0, 10000)]*24 + [(0, 5000)]*24 + [(0, 1000)]*5 + [(0, 2000)]*5\n",
        "x0 = np.array([np.mean(list(b_)) for b_ in b])\n",
        "# x0 = np.array([max(min(x_test[i]*(1+0.05*np.random.normal()),b[i][1]),0) for i in range(len(x_test))])\n",
        "dim = len(b)\n",
        "init_guess = dfo_f(x0)\n",
        "# optimum = dfo_f(x_test)\n",
        "print(init_guess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsgiFZXCVnES",
        "outputId": "23ad6a6b-7f78-44c5-9f3c-fe4c89da35e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "698.4034951593749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_e = 100\n",
        "budget = 10000\n",
        "\n",
        "t0 = time.time()\n",
        "test = minimize(\n",
        "    dfo_f, x0,\n",
        "    bounds=b,\n",
        "    method='Powell',\n",
        "    options={'maxfev': budget},\n",
        "    )\n",
        "t1 = time.time()\n",
        "print(f\"Test DFO optimum: {test['fun']:.3f} in {t1-t0:.3f} seconds starting from initial value of: {init_guess:.3f}\")\n",
        "\n",
        "t0 = time.time()\n",
        "dummy = optimizer_reduced(dfo_f, dim, b, N_e, budget)\n",
        "t = time.time()\n",
        "assert (t - t0 < 300), \"Ensure your optimizer finishes in 5 minutes\"\n",
        "print(f\"Your optimum: {dummy:.3f} in {t1-t0:.3f} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVBP_IKQVC8g",
        "outputId": "9c9f1652-fede-4317-bebf-103dcfc9c933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test DFO optimum: 14.963 in 69.952 seconds starting from initial value of: 698.403\n",
            "Your optimum: 698.403 in -0.002 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test functions (10 marks)"
      ],
      "metadata": {
        "id": "XflEk49qtUQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar test function with different data and different dimensionality"
      ],
      "metadata": {
        "id": "HUBHgD_M80lI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4W1c9R53B2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}